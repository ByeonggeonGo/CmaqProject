{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 721481021492663746\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3667263488\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 4838586376220672078\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "(119, 64, 64, 4)\n",
      "batch: 1/7.0 ---epoch: 0/100--- mse-loss: 177.98275756835938\n",
      "batch: 2/7.0 ---epoch: 0/100--- mse-loss: 173.122314453125\n",
      "batch: 3/7.0 ---epoch: 0/100--- mse-loss: 168.10574340820312\n",
      "batch: 4/7.0 ---epoch: 0/100--- mse-loss: 156.146240234375\n",
      "batch: 5/7.0 ---epoch: 0/100--- mse-loss: 144.01583862304688\n",
      "batch: 6/7.0 ---epoch: 0/100--- mse-loss: 123.22810363769531\n",
      "batch: 7/7.0 ---epoch: 0/100--- mse-loss: 103.6207504272461\n",
      "batch: 1/7.0 ---epoch: 1/100--- mse-loss: 98.0555648803711\n",
      "batch: 2/7.0 ---epoch: 1/100--- mse-loss: 101.467529296875\n",
      "batch: 3/7.0 ---epoch: 1/100--- mse-loss: 93.73999786376953\n",
      "batch: 4/7.0 ---epoch: 1/100--- mse-loss: 74.10103607177734\n",
      "batch: 5/7.0 ---epoch: 1/100--- mse-loss: 64.24554443359375\n",
      "batch: 6/7.0 ---epoch: 1/100--- mse-loss: 58.11696243286133\n",
      "batch: 7/7.0 ---epoch: 1/100--- mse-loss: 51.48189163208008\n",
      "batch: 1/7.0 ---epoch: 2/100--- mse-loss: 43.732887268066406\n",
      "batch: 2/7.0 ---epoch: 2/100--- mse-loss: 34.76946258544922\n",
      "batch: 3/7.0 ---epoch: 2/100--- mse-loss: 24.873821258544922\n",
      "batch: 4/7.0 ---epoch: 2/100--- mse-loss: 19.824880599975586\n",
      "batch: 5/7.0 ---epoch: 2/100--- mse-loss: 19.07269287109375\n",
      "batch: 6/7.0 ---epoch: 2/100--- mse-loss: 18.449403762817383\n",
      "batch: 7/7.0 ---epoch: 2/100--- mse-loss: 16.459575653076172\n",
      "batch: 1/7.0 ---epoch: 3/100--- mse-loss: 12.728960990905762\n",
      "batch: 2/7.0 ---epoch: 3/100--- mse-loss: 12.106186866760254\n",
      "batch: 3/7.0 ---epoch: 3/100--- mse-loss: 13.757787704467773\n",
      "batch: 4/7.0 ---epoch: 3/100--- mse-loss: 16.194169998168945\n",
      "batch: 5/7.0 ---epoch: 3/100--- mse-loss: 17.20136070251465\n",
      "batch: 6/7.0 ---epoch: 3/100--- mse-loss: 16.61447525024414\n",
      "batch: 7/7.0 ---epoch: 3/100--- mse-loss: 15.188733100891113\n",
      "batch: 1/7.0 ---epoch: 4/100--- mse-loss: 14.886198043823242\n",
      "batch: 2/7.0 ---epoch: 4/100--- mse-loss: 15.040842056274414\n",
      "batch: 3/7.0 ---epoch: 4/100--- mse-loss: 14.668914794921875\n",
      "batch: 4/7.0 ---epoch: 4/100--- mse-loss: 13.734731674194336\n",
      "batch: 5/7.0 ---epoch: 4/100--- mse-loss: 12.52890682220459\n",
      "batch: 6/7.0 ---epoch: 4/100--- mse-loss: 11.798776626586914\n",
      "batch: 7/7.0 ---epoch: 4/100--- mse-loss: 11.491040229797363\n",
      "batch: 1/7.0 ---epoch: 5/100--- mse-loss: 11.072120666503906\n",
      "batch: 2/7.0 ---epoch: 5/100--- mse-loss: 10.710298538208008\n",
      "batch: 3/7.0 ---epoch: 5/100--- mse-loss: 10.234383583068848\n",
      "batch: 4/7.0 ---epoch: 5/100--- mse-loss: 10.325706481933594\n",
      "batch: 5/7.0 ---epoch: 5/100--- mse-loss: 10.437848091125488\n",
      "batch: 6/7.0 ---epoch: 5/100--- mse-loss: 10.773374557495117\n",
      "batch: 7/7.0 ---epoch: 5/100--- mse-loss: 10.044127464294434\n",
      "batch: 1/7.0 ---epoch: 6/100--- mse-loss: 10.553033828735352\n",
      "batch: 2/7.0 ---epoch: 6/100--- mse-loss: 10.434422492980957\n",
      "batch: 3/7.0 ---epoch: 6/100--- mse-loss: 9.83860969543457\n",
      "batch: 4/7.0 ---epoch: 6/100--- mse-loss: 9.96435832977295\n",
      "batch: 5/7.0 ---epoch: 6/100--- mse-loss: 9.905294418334961\n",
      "batch: 6/7.0 ---epoch: 6/100--- mse-loss: 9.696876525878906\n",
      "batch: 7/7.0 ---epoch: 6/100--- mse-loss: 9.545450210571289\n",
      "batch: 1/7.0 ---epoch: 7/100--- mse-loss: 9.59526252746582\n",
      "batch: 2/7.0 ---epoch: 7/100--- mse-loss: 9.246564865112305\n",
      "batch: 3/7.0 ---epoch: 7/100--- mse-loss: 9.118334770202637\n",
      "batch: 4/7.0 ---epoch: 7/100--- mse-loss: 9.022623062133789\n",
      "batch: 5/7.0 ---epoch: 7/100--- mse-loss: 9.068319320678711\n",
      "batch: 6/7.0 ---epoch: 7/100--- mse-loss: 8.898172378540039\n",
      "batch: 7/7.0 ---epoch: 7/100--- mse-loss: 9.018754959106445\n",
      "batch: 1/7.0 ---epoch: 8/100--- mse-loss: 9.081212997436523\n",
      "batch: 2/7.0 ---epoch: 8/100--- mse-loss: 8.740215301513672\n",
      "batch: 3/7.0 ---epoch: 8/100--- mse-loss: 9.01630687713623\n",
      "batch: 4/7.0 ---epoch: 8/100--- mse-loss: 8.697990417480469\n",
      "batch: 5/7.0 ---epoch: 8/100--- mse-loss: 8.808182716369629\n",
      "batch: 6/7.0 ---epoch: 8/100--- mse-loss: 8.686117172241211\n",
      "batch: 7/7.0 ---epoch: 8/100--- mse-loss: 8.575114250183105\n",
      "batch: 1/7.0 ---epoch: 9/100--- mse-loss: 8.597356796264648\n",
      "batch: 2/7.0 ---epoch: 9/100--- mse-loss: 8.398143768310547\n",
      "batch: 3/7.0 ---epoch: 9/100--- mse-loss: 8.632170677185059\n",
      "batch: 4/7.0 ---epoch: 9/100--- mse-loss: 8.53364086151123\n",
      "batch: 5/7.0 ---epoch: 9/100--- mse-loss: 8.306323051452637\n",
      "batch: 6/7.0 ---epoch: 9/100--- mse-loss: 8.294400215148926\n",
      "batch: 7/7.0 ---epoch: 9/100--- mse-loss: 8.256958961486816\n",
      "batch: 1/7.0 ---epoch: 10/100--- mse-loss: 8.131214141845703\n",
      "batch: 2/7.0 ---epoch: 10/100--- mse-loss: 8.20898151397705\n",
      "batch: 3/7.0 ---epoch: 10/100--- mse-loss: 8.251054763793945\n",
      "batch: 4/7.0 ---epoch: 10/100--- mse-loss: 8.240877151489258\n",
      "batch: 5/7.0 ---epoch: 10/100--- mse-loss: 8.006695747375488\n",
      "batch: 6/7.0 ---epoch: 10/100--- mse-loss: 7.7853217124938965\n",
      "batch: 7/7.0 ---epoch: 10/100--- mse-loss: 8.013529777526855\n",
      "batch: 1/7.0 ---epoch: 11/100--- mse-loss: 7.741506099700928\n",
      "batch: 2/7.0 ---epoch: 11/100--- mse-loss: 7.903460502624512\n",
      "batch: 3/7.0 ---epoch: 11/100--- mse-loss: 7.681317329406738\n",
      "batch: 4/7.0 ---epoch: 11/100--- mse-loss: 7.780489444732666\n",
      "batch: 5/7.0 ---epoch: 11/100--- mse-loss: 7.783730983734131\n",
      "batch: 6/7.0 ---epoch: 11/100--- mse-loss: 7.833265781402588\n",
      "batch: 7/7.0 ---epoch: 11/100--- mse-loss: 7.590620517730713\n",
      "batch: 1/7.0 ---epoch: 12/100--- mse-loss: 7.4138689041137695\n",
      "batch: 2/7.0 ---epoch: 12/100--- mse-loss: 7.535405158996582\n",
      "batch: 3/7.0 ---epoch: 12/100--- mse-loss: 7.474287986755371\n",
      "batch: 4/7.0 ---epoch: 12/100--- mse-loss: 7.525811672210693\n",
      "batch: 5/7.0 ---epoch: 12/100--- mse-loss: 7.395759582519531\n",
      "batch: 6/7.0 ---epoch: 12/100--- mse-loss: 7.443772792816162\n",
      "batch: 7/7.0 ---epoch: 12/100--- mse-loss: 7.28618860244751\n",
      "batch: 1/7.0 ---epoch: 13/100--- mse-loss: 7.387742042541504\n",
      "batch: 2/7.0 ---epoch: 13/100--- mse-loss: 7.118177890777588\n",
      "batch: 3/7.0 ---epoch: 13/100--- mse-loss: 6.993517875671387\n",
      "batch: 4/7.0 ---epoch: 13/100--- mse-loss: 7.01540994644165\n",
      "batch: 5/7.0 ---epoch: 13/100--- mse-loss: 7.064688682556152\n",
      "batch: 6/7.0 ---epoch: 13/100--- mse-loss: 7.064134120941162\n",
      "batch: 7/7.0 ---epoch: 13/100--- mse-loss: 7.067723274230957\n",
      "batch: 1/7.0 ---epoch: 14/100--- mse-loss: 6.870526313781738\n",
      "batch: 2/7.0 ---epoch: 14/100--- mse-loss: 6.8305463790893555\n",
      "batch: 3/7.0 ---epoch: 14/100--- mse-loss: 6.8347578048706055\n",
      "batch: 4/7.0 ---epoch: 14/100--- mse-loss: 6.478005886077881\n",
      "batch: 5/7.0 ---epoch: 14/100--- mse-loss: 6.713747501373291\n",
      "batch: 6/7.0 ---epoch: 14/100--- mse-loss: 6.737172603607178\n",
      "batch: 7/7.0 ---epoch: 14/100--- mse-loss: 6.746737480163574\n",
      "batch: 1/7.0 ---epoch: 15/100--- mse-loss: 6.515777587890625\n",
      "batch: 2/7.0 ---epoch: 15/100--- mse-loss: 6.6334547996521\n",
      "batch: 3/7.0 ---epoch: 15/100--- mse-loss: 6.3960981369018555\n",
      "batch: 4/7.0 ---epoch: 15/100--- mse-loss: 6.361630439758301\n",
      "batch: 5/7.0 ---epoch: 15/100--- mse-loss: 6.446346282958984\n",
      "batch: 6/7.0 ---epoch: 15/100--- mse-loss: 6.230149745941162\n",
      "batch: 7/7.0 ---epoch: 15/100--- mse-loss: 6.3323869705200195\n",
      "batch: 1/7.0 ---epoch: 16/100--- mse-loss: 6.202420234680176\n",
      "batch: 2/7.0 ---epoch: 16/100--- mse-loss: 6.356761455535889\n",
      "batch: 3/7.0 ---epoch: 16/100--- mse-loss: 6.13489294052124\n",
      "batch: 4/7.0 ---epoch: 16/100--- mse-loss: 6.154208183288574\n",
      "batch: 5/7.0 ---epoch: 16/100--- mse-loss: 6.0112199783325195\n",
      "batch: 6/7.0 ---epoch: 16/100--- mse-loss: 6.02255916595459\n",
      "batch: 7/7.0 ---epoch: 16/100--- mse-loss: 5.902113914489746\n",
      "batch: 1/7.0 ---epoch: 17/100--- mse-loss: 5.824133396148682\n",
      "batch: 2/7.0 ---epoch: 17/100--- mse-loss: 5.865225315093994\n",
      "batch: 3/7.0 ---epoch: 17/100--- mse-loss: 5.924973487854004\n",
      "batch: 4/7.0 ---epoch: 17/100--- mse-loss: 5.949867248535156\n",
      "batch: 5/7.0 ---epoch: 17/100--- mse-loss: 5.841516971588135\n",
      "batch: 6/7.0 ---epoch: 17/100--- mse-loss: 5.831393718719482\n",
      "batch: 7/7.0 ---epoch: 17/100--- mse-loss: 5.70490026473999\n",
      "batch: 1/7.0 ---epoch: 18/100--- mse-loss: 5.765379905700684\n",
      "batch: 2/7.0 ---epoch: 18/100--- mse-loss: 5.758885383605957\n",
      "batch: 3/7.0 ---epoch: 18/100--- mse-loss: 5.641783714294434\n",
      "batch: 4/7.0 ---epoch: 18/100--- mse-loss: 5.5269012451171875\n",
      "batch: 5/7.0 ---epoch: 18/100--- mse-loss: 5.485641956329346\n",
      "batch: 6/7.0 ---epoch: 18/100--- mse-loss: 5.4802961349487305\n",
      "batch: 7/7.0 ---epoch: 18/100--- mse-loss: 5.631265163421631\n",
      "batch: 1/7.0 ---epoch: 19/100--- mse-loss: 5.44639253616333\n",
      "batch: 2/7.0 ---epoch: 19/100--- mse-loss: 5.465373992919922\n",
      "batch: 3/7.0 ---epoch: 19/100--- mse-loss: 5.412374019622803\n",
      "batch: 4/7.0 ---epoch: 19/100--- mse-loss: 5.303128719329834\n",
      "batch: 5/7.0 ---epoch: 19/100--- mse-loss: 5.522570610046387\n",
      "batch: 6/7.0 ---epoch: 19/100--- mse-loss: 5.391237735748291\n",
      "batch: 7/7.0 ---epoch: 19/100--- mse-loss: 5.2939043045043945\n",
      "batch: 1/7.0 ---epoch: 20/100--- mse-loss: 5.223329067230225\n",
      "batch: 2/7.0 ---epoch: 20/100--- mse-loss: 5.224248886108398\n",
      "batch: 3/7.0 ---epoch: 20/100--- mse-loss: 5.226080417633057\n",
      "batch: 4/7.0 ---epoch: 20/100--- mse-loss: 5.206974029541016\n",
      "batch: 5/7.0 ---epoch: 20/100--- mse-loss: 5.26901912689209\n",
      "batch: 6/7.0 ---epoch: 20/100--- mse-loss: 5.14742374420166\n",
      "batch: 7/7.0 ---epoch: 20/100--- mse-loss: 5.311903476715088\n",
      "batch: 1/7.0 ---epoch: 21/100--- mse-loss: 5.0819597244262695\n",
      "batch: 2/7.0 ---epoch: 21/100--- mse-loss: 5.136044502258301\n",
      "batch: 3/7.0 ---epoch: 21/100--- mse-loss: 5.100364685058594\n",
      "batch: 4/7.0 ---epoch: 21/100--- mse-loss: 5.045269966125488\n",
      "batch: 5/7.0 ---epoch: 21/100--- mse-loss: 5.133742332458496\n",
      "batch: 6/7.0 ---epoch: 21/100--- mse-loss: 4.992952823638916\n",
      "batch: 7/7.0 ---epoch: 21/100--- mse-loss: 5.086427688598633\n",
      "batch: 1/7.0 ---epoch: 22/100--- mse-loss: 4.926266670227051\n",
      "batch: 2/7.0 ---epoch: 22/100--- mse-loss: 5.021502494812012\n",
      "batch: 3/7.0 ---epoch: 22/100--- mse-loss: 5.012071132659912\n",
      "batch: 4/7.0 ---epoch: 22/100--- mse-loss: 4.9131364822387695\n",
      "batch: 5/7.0 ---epoch: 22/100--- mse-loss: 4.959559440612793\n",
      "batch: 6/7.0 ---epoch: 22/100--- mse-loss: 4.880060195922852\n",
      "batch: 7/7.0 ---epoch: 22/100--- mse-loss: 4.9559502601623535\n",
      "batch: 1/7.0 ---epoch: 23/100--- mse-loss: 4.764329433441162\n",
      "batch: 2/7.0 ---epoch: 23/100--- mse-loss: 4.851178169250488\n",
      "batch: 3/7.0 ---epoch: 23/100--- mse-loss: 4.932969093322754\n",
      "batch: 4/7.0 ---epoch: 23/100--- mse-loss: 4.8780646324157715\n",
      "batch: 5/7.0 ---epoch: 23/100--- mse-loss: 4.82219123840332\n",
      "batch: 6/7.0 ---epoch: 23/100--- mse-loss: 4.9008469581604\n",
      "batch: 7/7.0 ---epoch: 23/100--- mse-loss: 4.7455644607543945\n",
      "batch: 1/7.0 ---epoch: 24/100--- mse-loss: 4.876253604888916\n",
      "batch: 2/7.0 ---epoch: 24/100--- mse-loss: 4.720204830169678\n",
      "batch: 3/7.0 ---epoch: 24/100--- mse-loss: 4.709137916564941\n",
      "batch: 4/7.0 ---epoch: 24/100--- mse-loss: 4.683067321777344\n",
      "batch: 5/7.0 ---epoch: 24/100--- mse-loss: 4.748876094818115\n",
      "batch: 6/7.0 ---epoch: 24/100--- mse-loss: 4.823655605316162\n",
      "batch: 7/7.0 ---epoch: 24/100--- mse-loss: 4.623618125915527\n",
      "batch: 1/7.0 ---epoch: 25/100--- mse-loss: 4.675248622894287\n",
      "batch: 2/7.0 ---epoch: 25/100--- mse-loss: 4.654807090759277\n",
      "batch: 3/7.0 ---epoch: 25/100--- mse-loss: 4.616555690765381\n",
      "batch: 4/7.0 ---epoch: 25/100--- mse-loss: 4.560520172119141\n",
      "batch: 5/7.0 ---epoch: 25/100--- mse-loss: 4.5777411460876465\n",
      "batch: 6/7.0 ---epoch: 25/100--- mse-loss: 4.476990699768066\n",
      "batch: 7/7.0 ---epoch: 25/100--- mse-loss: 4.544944763183594\n",
      "batch: 1/7.0 ---epoch: 26/100--- mse-loss: 4.528228759765625\n",
      "batch: 2/7.0 ---epoch: 26/100--- mse-loss: 4.514537334442139\n",
      "batch: 3/7.0 ---epoch: 26/100--- mse-loss: 4.5001702308654785\n",
      "batch: 4/7.0 ---epoch: 26/100--- mse-loss: 4.530240058898926\n",
      "batch: 5/7.0 ---epoch: 26/100--- mse-loss: 4.439557075500488\n",
      "batch: 6/7.0 ---epoch: 26/100--- mse-loss: 4.5038862228393555\n",
      "batch: 7/7.0 ---epoch: 26/100--- mse-loss: 4.446296215057373\n",
      "batch: 1/7.0 ---epoch: 27/100--- mse-loss: 4.5238494873046875\n",
      "batch: 2/7.0 ---epoch: 27/100--- mse-loss: 4.427833080291748\n",
      "batch: 3/7.0 ---epoch: 27/100--- mse-loss: 4.441130638122559\n",
      "batch: 4/7.0 ---epoch: 27/100--- mse-loss: 4.355912685394287\n",
      "batch: 5/7.0 ---epoch: 27/100--- mse-loss: 4.400068759918213\n",
      "batch: 6/7.0 ---epoch: 27/100--- mse-loss: 4.353354454040527\n",
      "batch: 7/7.0 ---epoch: 27/100--- mse-loss: 4.374477386474609\n",
      "batch: 1/7.0 ---epoch: 28/100--- mse-loss: 4.3376970291137695\n",
      "batch: 2/7.0 ---epoch: 28/100--- mse-loss: 4.382870674133301\n",
      "batch: 3/7.0 ---epoch: 28/100--- mse-loss: 4.375786781311035\n",
      "batch: 4/7.0 ---epoch: 28/100--- mse-loss: 4.3467230796813965\n",
      "batch: 5/7.0 ---epoch: 28/100--- mse-loss: 4.335771560668945\n",
      "batch: 6/7.0 ---epoch: 28/100--- mse-loss: 4.280764102935791\n",
      "batch: 7/7.0 ---epoch: 28/100--- mse-loss: 4.278700828552246\n",
      "batch: 1/7.0 ---epoch: 29/100--- mse-loss: 4.262710094451904\n",
      "batch: 2/7.0 ---epoch: 29/100--- mse-loss: 4.2071380615234375\n",
      "batch: 3/7.0 ---epoch: 29/100--- mse-loss: 4.251293659210205\n",
      "batch: 4/7.0 ---epoch: 29/100--- mse-loss: 4.253340244293213\n",
      "batch: 5/7.0 ---epoch: 29/100--- mse-loss: 4.346184730529785\n",
      "batch: 6/7.0 ---epoch: 29/100--- mse-loss: 4.2249674797058105\n",
      "batch: 7/7.0 ---epoch: 29/100--- mse-loss: 4.329991340637207\n",
      "batch: 1/7.0 ---epoch: 30/100--- mse-loss: 4.347248077392578\n",
      "batch: 2/7.0 ---epoch: 30/100--- mse-loss: 4.218262672424316\n",
      "batch: 3/7.0 ---epoch: 30/100--- mse-loss: 4.26183557510376\n",
      "batch: 4/7.0 ---epoch: 30/100--- mse-loss: 4.282561302185059\n",
      "batch: 5/7.0 ---epoch: 30/100--- mse-loss: 4.181808948516846\n",
      "batch: 6/7.0 ---epoch: 30/100--- mse-loss: 4.2683210372924805\n",
      "batch: 7/7.0 ---epoch: 30/100--- mse-loss: 4.190438270568848\n",
      "batch: 1/7.0 ---epoch: 31/100--- mse-loss: 4.240460395812988\n",
      "batch: 2/7.0 ---epoch: 31/100--- mse-loss: 4.225520133972168\n",
      "batch: 3/7.0 ---epoch: 31/100--- mse-loss: 4.179970741271973\n",
      "batch: 4/7.0 ---epoch: 31/100--- mse-loss: 4.3634233474731445\n",
      "batch: 5/7.0 ---epoch: 31/100--- mse-loss: 4.247735977172852\n",
      "batch: 6/7.0 ---epoch: 31/100--- mse-loss: 4.161028861999512\n",
      "batch: 7/7.0 ---epoch: 31/100--- mse-loss: 4.281651020050049\n",
      "batch: 1/7.0 ---epoch: 32/100--- mse-loss: 4.167074203491211\n",
      "batch: 2/7.0 ---epoch: 32/100--- mse-loss: 4.190445423126221\n",
      "batch: 3/7.0 ---epoch: 32/100--- mse-loss: 4.12196159362793\n",
      "batch: 4/7.0 ---epoch: 32/100--- mse-loss: 4.201867580413818\n",
      "batch: 5/7.0 ---epoch: 32/100--- mse-loss: 4.095380783081055\n",
      "batch: 6/7.0 ---epoch: 32/100--- mse-loss: 4.08941650390625\n",
      "batch: 7/7.0 ---epoch: 32/100--- mse-loss: 4.0998945236206055\n",
      "batch: 1/7.0 ---epoch: 33/100--- mse-loss: 4.035050392150879\n",
      "batch: 2/7.0 ---epoch: 33/100--- mse-loss: 4.083409309387207\n",
      "batch: 3/7.0 ---epoch: 33/100--- mse-loss: 4.048292636871338\n",
      "batch: 4/7.0 ---epoch: 33/100--- mse-loss: 4.132043838500977\n",
      "batch: 5/7.0 ---epoch: 33/100--- mse-loss: 4.05173397064209\n",
      "batch: 6/7.0 ---epoch: 33/100--- mse-loss: 4.0862321853637695\n",
      "batch: 7/7.0 ---epoch: 33/100--- mse-loss: 4.109452247619629\n",
      "batch: 1/7.0 ---epoch: 34/100--- mse-loss: 4.015541076660156\n",
      "batch: 2/7.0 ---epoch: 34/100--- mse-loss: 3.960106611251831\n",
      "batch: 3/7.0 ---epoch: 34/100--- mse-loss: 4.083470344543457\n",
      "batch: 4/7.0 ---epoch: 34/100--- mse-loss: 4.029760360717773\n",
      "batch: 5/7.0 ---epoch: 34/100--- mse-loss: 4.127231597900391\n",
      "batch: 6/7.0 ---epoch: 34/100--- mse-loss: 3.948805332183838\n",
      "batch: 7/7.0 ---epoch: 34/100--- mse-loss: 4.102206230163574\n",
      "batch: 1/7.0 ---epoch: 35/100--- mse-loss: 4.05634880065918\n",
      "batch: 2/7.0 ---epoch: 35/100--- mse-loss: 3.9007155895233154\n",
      "batch: 3/7.0 ---epoch: 35/100--- mse-loss: 4.047962188720703\n",
      "batch: 4/7.0 ---epoch: 35/100--- mse-loss: 3.9802398681640625\n",
      "batch: 5/7.0 ---epoch: 35/100--- mse-loss: 3.9544360637664795\n",
      "batch: 6/7.0 ---epoch: 35/100--- mse-loss: 4.044754981994629\n",
      "batch: 7/7.0 ---epoch: 35/100--- mse-loss: 3.985238552093506\n",
      "batch: 1/7.0 ---epoch: 36/100--- mse-loss: 3.9994304180145264\n",
      "batch: 2/7.0 ---epoch: 36/100--- mse-loss: 3.9599616527557373\n",
      "batch: 3/7.0 ---epoch: 36/100--- mse-loss: 3.949831485748291\n",
      "batch: 4/7.0 ---epoch: 36/100--- mse-loss: 3.953814744949341\n",
      "batch: 5/7.0 ---epoch: 36/100--- mse-loss: 3.9083008766174316\n",
      "batch: 6/7.0 ---epoch: 36/100--- mse-loss: 3.969257354736328\n",
      "batch: 7/7.0 ---epoch: 36/100--- mse-loss: 3.8334197998046875\n",
      "batch: 1/7.0 ---epoch: 37/100--- mse-loss: 3.876701831817627\n",
      "batch: 2/7.0 ---epoch: 37/100--- mse-loss: 3.933546543121338\n",
      "batch: 3/7.0 ---epoch: 37/100--- mse-loss: 3.988253355026245\n",
      "batch: 4/7.0 ---epoch: 37/100--- mse-loss: 3.89428448677063\n",
      "batch: 5/7.0 ---epoch: 37/100--- mse-loss: 3.9153800010681152\n",
      "batch: 6/7.0 ---epoch: 37/100--- mse-loss: 3.840135097503662\n",
      "batch: 7/7.0 ---epoch: 37/100--- mse-loss: 3.9207096099853516\n",
      "batch: 1/7.0 ---epoch: 38/100--- mse-loss: 3.835253953933716\n",
      "batch: 2/7.0 ---epoch: 38/100--- mse-loss: 3.872386932373047\n",
      "batch: 3/7.0 ---epoch: 38/100--- mse-loss: 3.8407745361328125\n",
      "batch: 4/7.0 ---epoch: 38/100--- mse-loss: 3.85670804977417\n",
      "batch: 5/7.0 ---epoch: 38/100--- mse-loss: 3.967564105987549\n",
      "batch: 6/7.0 ---epoch: 38/100--- mse-loss: 3.9401862621307373\n",
      "batch: 7/7.0 ---epoch: 38/100--- mse-loss: 3.8263869285583496\n",
      "batch: 1/7.0 ---epoch: 39/100--- mse-loss: 3.8440723419189453\n",
      "batch: 2/7.0 ---epoch: 39/100--- mse-loss: 3.821866273880005\n",
      "batch: 3/7.0 ---epoch: 39/100--- mse-loss: 3.806762218475342\n",
      "batch: 4/7.0 ---epoch: 39/100--- mse-loss: 3.8889694213867188\n",
      "batch: 5/7.0 ---epoch: 39/100--- mse-loss: 3.768611431121826\n",
      "batch: 6/7.0 ---epoch: 39/100--- mse-loss: 3.864658832550049\n",
      "batch: 7/7.0 ---epoch: 39/100--- mse-loss: 3.7266361713409424\n",
      "batch: 1/7.0 ---epoch: 40/100--- mse-loss: 3.7733941078186035\n",
      "batch: 2/7.0 ---epoch: 40/100--- mse-loss: 3.755280017852783\n",
      "batch: 3/7.0 ---epoch: 40/100--- mse-loss: 3.800342559814453\n",
      "batch: 4/7.0 ---epoch: 40/100--- mse-loss: 3.812915086746216\n",
      "batch: 5/7.0 ---epoch: 40/100--- mse-loss: 3.7669570446014404\n",
      "batch: 6/7.0 ---epoch: 40/100--- mse-loss: 3.7920825481414795\n",
      "batch: 7/7.0 ---epoch: 40/100--- mse-loss: 3.7825019359588623\n",
      "batch: 1/7.0 ---epoch: 41/100--- mse-loss: 3.7723031044006348\n",
      "batch: 2/7.0 ---epoch: 41/100--- mse-loss: 3.8259987831115723\n",
      "batch: 3/7.0 ---epoch: 41/100--- mse-loss: 3.7401459217071533\n",
      "batch: 4/7.0 ---epoch: 41/100--- mse-loss: 3.767653226852417\n",
      "batch: 5/7.0 ---epoch: 41/100--- mse-loss: 3.7401134967803955\n",
      "batch: 6/7.0 ---epoch: 41/100--- mse-loss: 3.953545331954956\n",
      "batch: 7/7.0 ---epoch: 41/100--- mse-loss: 3.80012583732605\n",
      "batch: 1/7.0 ---epoch: 42/100--- mse-loss: 3.715916395187378\n",
      "batch: 2/7.0 ---epoch: 42/100--- mse-loss: 3.7862930297851562\n",
      "batch: 3/7.0 ---epoch: 42/100--- mse-loss: 3.7373695373535156\n",
      "batch: 4/7.0 ---epoch: 42/100--- mse-loss: 3.6949448585510254\n",
      "batch: 5/7.0 ---epoch: 42/100--- mse-loss: 3.723193407058716\n",
      "batch: 6/7.0 ---epoch: 42/100--- mse-loss: 3.6889986991882324\n",
      "batch: 7/7.0 ---epoch: 42/100--- mse-loss: 3.7109673023223877\n",
      "batch: 1/7.0 ---epoch: 43/100--- mse-loss: 3.7313027381896973\n",
      "batch: 2/7.0 ---epoch: 43/100--- mse-loss: 3.709857940673828\n",
      "batch: 3/7.0 ---epoch: 43/100--- mse-loss: 3.6970951557159424\n",
      "batch: 4/7.0 ---epoch: 43/100--- mse-loss: 3.6569976806640625\n",
      "batch: 5/7.0 ---epoch: 43/100--- mse-loss: 3.714398145675659\n",
      "batch: 6/7.0 ---epoch: 43/100--- mse-loss: 3.7281875610351562\n",
      "batch: 7/7.0 ---epoch: 43/100--- mse-loss: 3.6515891551971436\n",
      "batch: 1/7.0 ---epoch: 44/100--- mse-loss: 3.7120978832244873\n",
      "batch: 2/7.0 ---epoch: 44/100--- mse-loss: 3.6864333152770996\n",
      "batch: 3/7.0 ---epoch: 44/100--- mse-loss: 3.6272239685058594\n",
      "batch: 4/7.0 ---epoch: 44/100--- mse-loss: 3.703878402709961\n",
      "batch: 5/7.0 ---epoch: 44/100--- mse-loss: 3.713003158569336\n",
      "batch: 6/7.0 ---epoch: 44/100--- mse-loss: 3.7086830139160156\n",
      "batch: 7/7.0 ---epoch: 44/100--- mse-loss: 3.749886989593506\n",
      "batch: 1/7.0 ---epoch: 45/100--- mse-loss: 3.6293177604675293\n",
      "batch: 2/7.0 ---epoch: 45/100--- mse-loss: 3.678873062133789\n",
      "batch: 3/7.0 ---epoch: 45/100--- mse-loss: 3.684138774871826\n",
      "batch: 4/7.0 ---epoch: 45/100--- mse-loss: 3.6201529502868652\n",
      "batch: 5/7.0 ---epoch: 45/100--- mse-loss: 3.624912977218628\n",
      "batch: 6/7.0 ---epoch: 45/100--- mse-loss: 3.703819751739502\n",
      "batch: 7/7.0 ---epoch: 45/100--- mse-loss: 3.569072723388672\n",
      "batch: 1/7.0 ---epoch: 46/100--- mse-loss: 3.6423373222351074\n",
      "batch: 2/7.0 ---epoch: 46/100--- mse-loss: 3.6613917350769043\n",
      "batch: 3/7.0 ---epoch: 46/100--- mse-loss: 3.763457775115967\n",
      "batch: 4/7.0 ---epoch: 46/100--- mse-loss: 3.698373794555664\n",
      "batch: 5/7.0 ---epoch: 46/100--- mse-loss: 3.6203644275665283\n",
      "batch: 6/7.0 ---epoch: 46/100--- mse-loss: 3.697523593902588\n",
      "batch: 7/7.0 ---epoch: 46/100--- mse-loss: 3.9039626121520996\n",
      "batch: 1/7.0 ---epoch: 47/100--- mse-loss: 3.66410756111145\n",
      "batch: 2/7.0 ---epoch: 47/100--- mse-loss: 3.567356824874878\n",
      "batch: 3/7.0 ---epoch: 47/100--- mse-loss: 3.715723752975464\n",
      "batch: 4/7.0 ---epoch: 47/100--- mse-loss: 3.850537061691284\n",
      "batch: 5/7.0 ---epoch: 47/100--- mse-loss: 3.643216609954834\n",
      "batch: 6/7.0 ---epoch: 47/100--- mse-loss: 3.6241488456726074\n",
      "batch: 7/7.0 ---epoch: 47/100--- mse-loss: 3.623572587966919\n",
      "batch: 1/7.0 ---epoch: 48/100--- mse-loss: 3.631833553314209\n",
      "batch: 2/7.0 ---epoch: 48/100--- mse-loss: 3.5956192016601562\n",
      "batch: 3/7.0 ---epoch: 48/100--- mse-loss: 3.5836663246154785\n",
      "batch: 4/7.0 ---epoch: 48/100--- mse-loss: 3.667497158050537\n",
      "batch: 5/7.0 ---epoch: 48/100--- mse-loss: 3.5895142555236816\n",
      "batch: 6/7.0 ---epoch: 48/100--- mse-loss: 3.5272457599639893\n",
      "batch: 7/7.0 ---epoch: 48/100--- mse-loss: 3.6402320861816406\n",
      "batch: 1/7.0 ---epoch: 49/100--- mse-loss: 3.5813841819763184\n",
      "batch: 2/7.0 ---epoch: 49/100--- mse-loss: 3.587996244430542\n",
      "batch: 3/7.0 ---epoch: 49/100--- mse-loss: 3.572897434234619\n",
      "batch: 4/7.0 ---epoch: 49/100--- mse-loss: 3.664473295211792\n",
      "batch: 5/7.0 ---epoch: 49/100--- mse-loss: 3.6178078651428223\n",
      "batch: 6/7.0 ---epoch: 49/100--- mse-loss: 3.5024495124816895\n",
      "batch: 7/7.0 ---epoch: 49/100--- mse-loss: 3.4959728717803955\n",
      "batch: 1/7.0 ---epoch: 50/100--- mse-loss: 3.5050933361053467\n",
      "batch: 2/7.0 ---epoch: 50/100--- mse-loss: 3.590010166168213\n",
      "batch: 3/7.0 ---epoch: 50/100--- mse-loss: 3.543497085571289\n",
      "batch: 4/7.0 ---epoch: 50/100--- mse-loss: 3.576963424682617\n",
      "batch: 5/7.0 ---epoch: 50/100--- mse-loss: 3.6254990100860596\n",
      "batch: 6/7.0 ---epoch: 50/100--- mse-loss: 3.5337233543395996\n",
      "batch: 7/7.0 ---epoch: 50/100--- mse-loss: 3.4912033081054688\n",
      "batch: 1/7.0 ---epoch: 51/100--- mse-loss: 3.5005950927734375\n",
      "batch: 2/7.0 ---epoch: 51/100--- mse-loss: 3.5220863819122314\n",
      "batch: 3/7.0 ---epoch: 51/100--- mse-loss: 3.4932327270507812\n",
      "batch: 4/7.0 ---epoch: 51/100--- mse-loss: 3.489436626434326\n",
      "batch: 5/7.0 ---epoch: 51/100--- mse-loss: 3.569819927215576\n",
      "batch: 6/7.0 ---epoch: 51/100--- mse-loss: 3.5533127784729004\n",
      "batch: 7/7.0 ---epoch: 51/100--- mse-loss: 3.405625581741333\n",
      "batch: 1/7.0 ---epoch: 52/100--- mse-loss: 3.5470504760742188\n",
      "batch: 2/7.0 ---epoch: 52/100--- mse-loss: 3.4865200519561768\n",
      "batch: 3/7.0 ---epoch: 52/100--- mse-loss: 3.511920213699341\n",
      "batch: 4/7.0 ---epoch: 52/100--- mse-loss: 3.523700714111328\n",
      "batch: 5/7.0 ---epoch: 52/100--- mse-loss: 3.4769744873046875\n",
      "batch: 6/7.0 ---epoch: 52/100--- mse-loss: 3.419623851776123\n",
      "batch: 7/7.0 ---epoch: 52/100--- mse-loss: 3.4783687591552734\n",
      "batch: 1/7.0 ---epoch: 53/100--- mse-loss: 3.505774736404419\n",
      "batch: 2/7.0 ---epoch: 53/100--- mse-loss: 3.546858310699463\n",
      "batch: 3/7.0 ---epoch: 53/100--- mse-loss: 3.4851536750793457\n",
      "batch: 4/7.0 ---epoch: 53/100--- mse-loss: 3.449213743209839\n",
      "batch: 5/7.0 ---epoch: 53/100--- mse-loss: 3.409360408782959\n",
      "batch: 6/7.0 ---epoch: 53/100--- mse-loss: 3.4720046520233154\n",
      "batch: 7/7.0 ---epoch: 53/100--- mse-loss: 3.438833713531494\n",
      "batch: 1/7.0 ---epoch: 54/100--- mse-loss: 3.4872772693634033\n",
      "batch: 2/7.0 ---epoch: 54/100--- mse-loss: 3.5232558250427246\n",
      "batch: 3/7.0 ---epoch: 54/100--- mse-loss: 3.6952290534973145\n",
      "batch: 4/7.0 ---epoch: 54/100--- mse-loss: 3.6173133850097656\n",
      "batch: 5/7.0 ---epoch: 54/100--- mse-loss: 3.5206406116485596\n",
      "batch: 6/7.0 ---epoch: 54/100--- mse-loss: 3.391273021697998\n",
      "batch: 7/7.0 ---epoch: 54/100--- mse-loss: 3.573286533355713\n",
      "batch: 1/7.0 ---epoch: 55/100--- mse-loss: 3.8185760974884033\n",
      "batch: 2/7.0 ---epoch: 55/100--- mse-loss: 3.9813408851623535\n",
      "batch: 3/7.0 ---epoch: 55/100--- mse-loss: 3.6869215965270996\n",
      "batch: 4/7.0 ---epoch: 55/100--- mse-loss: 3.4076087474823\n",
      "batch: 5/7.0 ---epoch: 55/100--- mse-loss: 3.5007877349853516\n",
      "batch: 6/7.0 ---epoch: 55/100--- mse-loss: 3.5141944885253906\n",
      "batch: 7/7.0 ---epoch: 55/100--- mse-loss: 3.461348056793213\n",
      "batch: 1/7.0 ---epoch: 56/100--- mse-loss: 3.5590567588806152\n",
      "batch: 2/7.0 ---epoch: 56/100--- mse-loss: 3.6616768836975098\n",
      "batch: 3/7.0 ---epoch: 56/100--- mse-loss: 3.3967812061309814\n",
      "batch: 4/7.0 ---epoch: 56/100--- mse-loss: 3.514247179031372\n",
      "batch: 5/7.0 ---epoch: 56/100--- mse-loss: 4.020240306854248\n",
      "batch: 6/7.0 ---epoch: 56/100--- mse-loss: 4.0119948387146\n",
      "batch: 7/7.0 ---epoch: 56/100--- mse-loss: 3.4976744651794434\n",
      "batch: 1/7.0 ---epoch: 57/100--- mse-loss: 3.627789258956909\n",
      "batch: 2/7.0 ---epoch: 57/100--- mse-loss: 3.874345064163208\n",
      "batch: 3/7.0 ---epoch: 57/100--- mse-loss: 3.5957324504852295\n",
      "batch: 4/7.0 ---epoch: 57/100--- mse-loss: 3.4993832111358643\n",
      "batch: 5/7.0 ---epoch: 57/100--- mse-loss: 3.8336594104766846\n",
      "batch: 6/7.0 ---epoch: 57/100--- mse-loss: 3.5157768726348877\n",
      "batch: 7/7.0 ---epoch: 57/100--- mse-loss: 3.427854061126709\n",
      "batch: 1/7.0 ---epoch: 58/100--- mse-loss: 3.5937981605529785\n",
      "batch: 2/7.0 ---epoch: 58/100--- mse-loss: 3.4118735790252686\n",
      "batch: 3/7.0 ---epoch: 58/100--- mse-loss: 3.4095184803009033\n",
      "batch: 4/7.0 ---epoch: 58/100--- mse-loss: 3.5755996704101562\n",
      "batch: 5/7.0 ---epoch: 58/100--- mse-loss: 3.5501651763916016\n",
      "batch: 6/7.0 ---epoch: 58/100--- mse-loss: 3.3989853858947754\n",
      "batch: 7/7.0 ---epoch: 58/100--- mse-loss: 3.4713892936706543\n",
      "batch: 1/7.0 ---epoch: 59/100--- mse-loss: 3.4396812915802\n",
      "batch: 2/7.0 ---epoch: 59/100--- mse-loss: 3.38545298576355\n",
      "batch: 3/7.0 ---epoch: 59/100--- mse-loss: 3.546722412109375\n",
      "batch: 4/7.0 ---epoch: 59/100--- mse-loss: 3.4673447608947754\n",
      "batch: 5/7.0 ---epoch: 59/100--- mse-loss: 3.429719924926758\n",
      "batch: 6/7.0 ---epoch: 59/100--- mse-loss: 3.674187421798706\n",
      "batch: 7/7.0 ---epoch: 59/100--- mse-loss: 3.470745086669922\n",
      "batch: 1/7.0 ---epoch: 60/100--- mse-loss: 3.404142379760742\n",
      "batch: 2/7.0 ---epoch: 60/100--- mse-loss: 3.465717315673828\n",
      "batch: 3/7.0 ---epoch: 60/100--- mse-loss: 3.3605034351348877\n",
      "batch: 4/7.0 ---epoch: 60/100--- mse-loss: 3.40820050239563\n",
      "batch: 5/7.0 ---epoch: 60/100--- mse-loss: 3.3651721477508545\n",
      "batch: 6/7.0 ---epoch: 60/100--- mse-loss: 3.3718109130859375\n",
      "batch: 7/7.0 ---epoch: 60/100--- mse-loss: 3.4471077919006348\n",
      "batch: 1/7.0 ---epoch: 61/100--- mse-loss: 3.3820266723632812\n",
      "batch: 2/7.0 ---epoch: 61/100--- mse-loss: 3.3691372871398926\n",
      "batch: 3/7.0 ---epoch: 61/100--- mse-loss: 3.4082512855529785\n",
      "batch: 4/7.0 ---epoch: 61/100--- mse-loss: 3.4112727642059326\n",
      "batch: 5/7.0 ---epoch: 61/100--- mse-loss: 3.3805413246154785\n",
      "batch: 6/7.0 ---epoch: 61/100--- mse-loss: 3.3691272735595703\n",
      "batch: 7/7.0 ---epoch: 61/100--- mse-loss: 3.3758106231689453\n",
      "batch: 1/7.0 ---epoch: 62/100--- mse-loss: 3.3064582347869873\n",
      "batch: 2/7.0 ---epoch: 62/100--- mse-loss: 3.3683173656463623\n",
      "batch: 3/7.0 ---epoch: 62/100--- mse-loss: 3.339721202850342\n",
      "batch: 4/7.0 ---epoch: 62/100--- mse-loss: 3.306863784790039\n",
      "batch: 5/7.0 ---epoch: 62/100--- mse-loss: 3.4051246643066406\n",
      "batch: 6/7.0 ---epoch: 62/100--- mse-loss: 3.3663933277130127\n",
      "batch: 7/7.0 ---epoch: 62/100--- mse-loss: 3.3180413246154785\n",
      "batch: 1/7.0 ---epoch: 63/100--- mse-loss: 3.3769145011901855\n",
      "batch: 2/7.0 ---epoch: 63/100--- mse-loss: 3.3793091773986816\n",
      "batch: 3/7.0 ---epoch: 63/100--- mse-loss: 3.3264987468719482\n",
      "batch: 4/7.0 ---epoch: 63/100--- mse-loss: 3.374267578125\n",
      "batch: 5/7.0 ---epoch: 63/100--- mse-loss: 3.32722806930542\n",
      "batch: 6/7.0 ---epoch: 63/100--- mse-loss: 3.307628631591797\n",
      "batch: 7/7.0 ---epoch: 63/100--- mse-loss: 3.3614799976348877\n",
      "batch: 1/7.0 ---epoch: 64/100--- mse-loss: 3.412525177001953\n",
      "batch: 2/7.0 ---epoch: 64/100--- mse-loss: 3.4253158569335938\n",
      "batch: 3/7.0 ---epoch: 64/100--- mse-loss: 3.360055446624756\n",
      "batch: 4/7.0 ---epoch: 64/100--- mse-loss: 3.246523380279541\n",
      "batch: 5/7.0 ---epoch: 64/100--- mse-loss: 3.340789794921875\n",
      "batch: 6/7.0 ---epoch: 64/100--- mse-loss: 3.289503812789917\n",
      "batch: 7/7.0 ---epoch: 64/100--- mse-loss: 3.3571815490722656\n",
      "batch: 1/7.0 ---epoch: 65/100--- mse-loss: 3.3103370666503906\n",
      "batch: 2/7.0 ---epoch: 65/100--- mse-loss: 3.3534798622131348\n",
      "batch: 3/7.0 ---epoch: 65/100--- mse-loss: 3.4461569786071777\n",
      "batch: 4/7.0 ---epoch: 65/100--- mse-loss: 3.436379909515381\n",
      "batch: 5/7.0 ---epoch: 65/100--- mse-loss: 3.3283145427703857\n",
      "batch: 6/7.0 ---epoch: 65/100--- mse-loss: 3.230360507965088\n",
      "batch: 7/7.0 ---epoch: 65/100--- mse-loss: 3.3600239753723145\n",
      "batch: 1/7.0 ---epoch: 66/100--- mse-loss: 3.388641357421875\n",
      "batch: 2/7.0 ---epoch: 66/100--- mse-loss: 3.2843430042266846\n",
      "batch: 3/7.0 ---epoch: 66/100--- mse-loss: 3.3640594482421875\n",
      "batch: 4/7.0 ---epoch: 66/100--- mse-loss: 3.3906493186950684\n",
      "batch: 5/7.0 ---epoch: 66/100--- mse-loss: 3.3331291675567627\n",
      "batch: 6/7.0 ---epoch: 66/100--- mse-loss: 3.253201961517334\n",
      "batch: 7/7.0 ---epoch: 66/100--- mse-loss: 3.3046557903289795\n",
      "batch: 1/7.0 ---epoch: 67/100--- mse-loss: 3.2993721961975098\n",
      "batch: 2/7.0 ---epoch: 67/100--- mse-loss: 3.315113067626953\n",
      "batch: 3/7.0 ---epoch: 67/100--- mse-loss: 3.339216709136963\n",
      "batch: 4/7.0 ---epoch: 67/100--- mse-loss: 3.3158812522888184\n",
      "batch: 5/7.0 ---epoch: 67/100--- mse-loss: 3.4229416847229004\n",
      "batch: 6/7.0 ---epoch: 67/100--- mse-loss: 3.2888190746307373\n",
      "batch: 7/7.0 ---epoch: 67/100--- mse-loss: 3.318815231323242\n",
      "batch: 1/7.0 ---epoch: 68/100--- mse-loss: 3.299433946609497\n",
      "batch: 2/7.0 ---epoch: 68/100--- mse-loss: 3.2773914337158203\n",
      "batch: 3/7.0 ---epoch: 68/100--- mse-loss: 3.3424155712127686\n",
      "batch: 4/7.0 ---epoch: 68/100--- mse-loss: 3.3301587104797363\n",
      "batch: 5/7.0 ---epoch: 68/100--- mse-loss: 3.2734427452087402\n",
      "batch: 6/7.0 ---epoch: 68/100--- mse-loss: 3.5016701221466064\n",
      "batch: 7/7.0 ---epoch: 68/100--- mse-loss: 3.6236748695373535\n",
      "batch: 1/7.0 ---epoch: 69/100--- mse-loss: 3.3781611919403076\n",
      "batch: 2/7.0 ---epoch: 69/100--- mse-loss: 3.336216688156128\n",
      "batch: 3/7.0 ---epoch: 69/100--- mse-loss: 3.4881796836853027\n",
      "batch: 4/7.0 ---epoch: 69/100--- mse-loss: 3.5221962928771973\n",
      "batch: 5/7.0 ---epoch: 69/100--- mse-loss: 3.3124427795410156\n",
      "batch: 6/7.0 ---epoch: 69/100--- mse-loss: 3.3570239543914795\n",
      "batch: 7/7.0 ---epoch: 69/100--- mse-loss: 3.390768051147461\n",
      "batch: 1/7.0 ---epoch: 70/100--- mse-loss: 3.279369354248047\n",
      "batch: 2/7.0 ---epoch: 70/100--- mse-loss: 3.345221757888794\n",
      "batch: 3/7.0 ---epoch: 70/100--- mse-loss: 3.333681583404541\n",
      "batch: 4/7.0 ---epoch: 70/100--- mse-loss: 3.285475492477417\n",
      "batch: 5/7.0 ---epoch: 70/100--- mse-loss: 3.309816837310791\n",
      "batch: 6/7.0 ---epoch: 70/100--- mse-loss: 3.363093137741089\n",
      "batch: 7/7.0 ---epoch: 70/100--- mse-loss: 3.223736524581909\n",
      "batch: 1/7.0 ---epoch: 71/100--- mse-loss: 3.2919516563415527\n",
      "batch: 2/7.0 ---epoch: 71/100--- mse-loss: 3.283827304840088\n",
      "batch: 3/7.0 ---epoch: 71/100--- mse-loss: 3.27668833732605\n",
      "batch: 4/7.0 ---epoch: 71/100--- mse-loss: 3.2302050590515137\n",
      "batch: 5/7.0 ---epoch: 71/100--- mse-loss: 3.2298500537872314\n",
      "batch: 6/7.0 ---epoch: 71/100--- mse-loss: 3.258007049560547\n",
      "batch: 7/7.0 ---epoch: 71/100--- mse-loss: 3.2671217918395996\n",
      "batch: 1/7.0 ---epoch: 72/100--- mse-loss: 3.240605592727661\n",
      "batch: 2/7.0 ---epoch: 72/100--- mse-loss: 3.3354034423828125\n",
      "batch: 3/7.0 ---epoch: 72/100--- mse-loss: 3.293288469314575\n",
      "batch: 4/7.0 ---epoch: 72/100--- mse-loss: 3.237020492553711\n",
      "batch: 5/7.0 ---epoch: 72/100--- mse-loss: 3.1793930530548096\n",
      "batch: 6/7.0 ---epoch: 72/100--- mse-loss: 3.2140183448791504\n",
      "batch: 7/7.0 ---epoch: 72/100--- mse-loss: 3.2750580310821533\n",
      "batch: 1/7.0 ---epoch: 73/100--- mse-loss: 3.2801003456115723\n",
      "batch: 2/7.0 ---epoch: 73/100--- mse-loss: 3.2567031383514404\n",
      "batch: 3/7.0 ---epoch: 73/100--- mse-loss: 3.273197889328003\n",
      "batch: 4/7.0 ---epoch: 73/100--- mse-loss: 3.2417092323303223\n",
      "batch: 5/7.0 ---epoch: 73/100--- mse-loss: 3.2796294689178467\n",
      "batch: 6/7.0 ---epoch: 73/100--- mse-loss: 3.2187633514404297\n",
      "batch: 7/7.0 ---epoch: 73/100--- mse-loss: 3.216036319732666\n",
      "batch: 1/7.0 ---epoch: 74/100--- mse-loss: 3.235008716583252\n",
      "batch: 2/7.0 ---epoch: 74/100--- mse-loss: 3.216722011566162\n",
      "batch: 3/7.0 ---epoch: 74/100--- mse-loss: 3.355776309967041\n",
      "batch: 4/7.0 ---epoch: 74/100--- mse-loss: 3.471764326095581\n",
      "batch: 5/7.0 ---epoch: 74/100--- mse-loss: 3.510096788406372\n",
      "batch: 6/7.0 ---epoch: 74/100--- mse-loss: 3.276521682739258\n",
      "batch: 7/7.0 ---epoch: 74/100--- mse-loss: 3.2592220306396484\n",
      "batch: 1/7.0 ---epoch: 75/100--- mse-loss: 3.3710098266601562\n",
      "batch: 2/7.0 ---epoch: 75/100--- mse-loss: 3.4557464122772217\n",
      "batch: 3/7.0 ---epoch: 75/100--- mse-loss: 3.2637195587158203\n",
      "batch: 4/7.0 ---epoch: 75/100--- mse-loss: 3.210601806640625\n",
      "batch: 5/7.0 ---epoch: 75/100--- mse-loss: 3.3265633583068848\n",
      "batch: 6/7.0 ---epoch: 75/100--- mse-loss: 3.298440933227539\n",
      "batch: 7/7.0 ---epoch: 75/100--- mse-loss: 3.205954074859619\n",
      "batch: 1/7.0 ---epoch: 76/100--- mse-loss: 3.2344138622283936\n",
      "batch: 2/7.0 ---epoch: 76/100--- mse-loss: 3.23956298828125\n",
      "batch: 3/7.0 ---epoch: 76/100--- mse-loss: 3.23272705078125\n",
      "batch: 4/7.0 ---epoch: 76/100--- mse-loss: 3.20154070854187\n",
      "batch: 5/7.0 ---epoch: 76/100--- mse-loss: 3.238966464996338\n",
      "batch: 6/7.0 ---epoch: 76/100--- mse-loss: 3.162849187850952\n",
      "batch: 7/7.0 ---epoch: 76/100--- mse-loss: 3.2465388774871826\n",
      "batch: 1/7.0 ---epoch: 77/100--- mse-loss: 3.2508225440979004\n",
      "batch: 2/7.0 ---epoch: 77/100--- mse-loss: 3.205832004547119\n",
      "batch: 3/7.0 ---epoch: 77/100--- mse-loss: 3.314370632171631\n",
      "batch: 4/7.0 ---epoch: 77/100--- mse-loss: 3.2174296379089355\n",
      "batch: 5/7.0 ---epoch: 77/100--- mse-loss: 3.195742130279541\n",
      "batch: 6/7.0 ---epoch: 77/100--- mse-loss: 3.248549699783325\n",
      "batch: 7/7.0 ---epoch: 77/100--- mse-loss: 3.25457763671875\n",
      "batch: 1/7.0 ---epoch: 78/100--- mse-loss: 3.2520909309387207\n",
      "batch: 2/7.0 ---epoch: 78/100--- mse-loss: 3.1952097415924072\n",
      "batch: 3/7.0 ---epoch: 78/100--- mse-loss: 3.2577481269836426\n",
      "batch: 4/7.0 ---epoch: 78/100--- mse-loss: 3.352729082107544\n",
      "batch: 5/7.0 ---epoch: 78/100--- mse-loss: 3.2555718421936035\n",
      "batch: 6/7.0 ---epoch: 78/100--- mse-loss: 3.18607497215271\n",
      "batch: 7/7.0 ---epoch: 78/100--- mse-loss: 3.2116780281066895\n",
      "batch: 1/7.0 ---epoch: 79/100--- mse-loss: 3.2584433555603027\n",
      "batch: 2/7.0 ---epoch: 79/100--- mse-loss: 3.3058223724365234\n",
      "batch: 3/7.0 ---epoch: 79/100--- mse-loss: 3.234065294265747\n",
      "batch: 4/7.0 ---epoch: 79/100--- mse-loss: 3.2146849632263184\n",
      "batch: 5/7.0 ---epoch: 79/100--- mse-loss: 3.3538403511047363\n",
      "batch: 6/7.0 ---epoch: 79/100--- mse-loss: 3.302476167678833\n",
      "batch: 7/7.0 ---epoch: 79/100--- mse-loss: 3.2077765464782715\n",
      "batch: 1/7.0 ---epoch: 80/100--- mse-loss: 3.265603542327881\n",
      "batch: 2/7.0 ---epoch: 80/100--- mse-loss: 3.5079169273376465\n",
      "batch: 3/7.0 ---epoch: 80/100--- mse-loss: 3.5719943046569824\n",
      "batch: 4/7.0 ---epoch: 80/100--- mse-loss: 3.3487515449523926\n",
      "batch: 5/7.0 ---epoch: 80/100--- mse-loss: 3.1704108715057373\n",
      "batch: 6/7.0 ---epoch: 80/100--- mse-loss: 3.1748642921447754\n",
      "batch: 7/7.0 ---epoch: 80/100--- mse-loss: 3.3692660331726074\n",
      "batch: 1/7.0 ---epoch: 81/100--- mse-loss: 3.354036808013916\n",
      "batch: 2/7.0 ---epoch: 81/100--- mse-loss: 3.1341800689697266\n",
      "batch: 3/7.0 ---epoch: 81/100--- mse-loss: 3.368192672729492\n",
      "batch: 4/7.0 ---epoch: 81/100--- mse-loss: 3.6554458141326904\n",
      "batch: 5/7.0 ---epoch: 81/100--- mse-loss: 3.4503417015075684\n",
      "batch: 6/7.0 ---epoch: 81/100--- mse-loss: 3.1977767944335938\n",
      "batch: 7/7.0 ---epoch: 81/100--- mse-loss: 3.475306749343872\n",
      "batch: 1/7.0 ---epoch: 82/100--- mse-loss: 3.5358567237854004\n",
      "batch: 2/7.0 ---epoch: 82/100--- mse-loss: 3.215211868286133\n",
      "batch: 3/7.0 ---epoch: 82/100--- mse-loss: 3.265291929244995\n",
      "batch: 4/7.0 ---epoch: 82/100--- mse-loss: 3.3692450523376465\n",
      "batch: 5/7.0 ---epoch: 82/100--- mse-loss: 3.22039794921875\n",
      "batch: 6/7.0 ---epoch: 82/100--- mse-loss: 3.286728620529175\n",
      "batch: 7/7.0 ---epoch: 82/100--- mse-loss: 3.4289374351501465\n",
      "batch: 1/7.0 ---epoch: 83/100--- mse-loss: 3.3990821838378906\n",
      "batch: 2/7.0 ---epoch: 83/100--- mse-loss: 3.204702377319336\n",
      "batch: 3/7.0 ---epoch: 83/100--- mse-loss: 3.180154800415039\n",
      "batch: 4/7.0 ---epoch: 83/100--- mse-loss: 3.246176242828369\n",
      "batch: 5/7.0 ---epoch: 83/100--- mse-loss: 3.1639280319213867\n",
      "batch: 6/7.0 ---epoch: 83/100--- mse-loss: 3.176861047744751\n",
      "batch: 7/7.0 ---epoch: 83/100--- mse-loss: 3.1776938438415527\n",
      "batch: 1/7.0 ---epoch: 84/100--- mse-loss: 3.2580745220184326\n",
      "batch: 2/7.0 ---epoch: 84/100--- mse-loss: 3.2498862743377686\n",
      "batch: 3/7.0 ---epoch: 84/100--- mse-loss: 3.1591365337371826\n",
      "batch: 4/7.0 ---epoch: 84/100--- mse-loss: 3.2147719860076904\n",
      "batch: 5/7.0 ---epoch: 84/100--- mse-loss: 3.2792301177978516\n",
      "batch: 6/7.0 ---epoch: 84/100--- mse-loss: 3.196967601776123\n",
      "batch: 7/7.0 ---epoch: 84/100--- mse-loss: 3.210775375366211\n",
      "batch: 1/7.0 ---epoch: 85/100--- mse-loss: 3.275099992752075\n",
      "batch: 2/7.0 ---epoch: 85/100--- mse-loss: 3.3255836963653564\n",
      "batch: 3/7.0 ---epoch: 85/100--- mse-loss: 3.1202926635742188\n",
      "batch: 4/7.0 ---epoch: 85/100--- mse-loss: 3.1826815605163574\n",
      "batch: 5/7.0 ---epoch: 85/100--- mse-loss: 3.3396828174591064\n",
      "batch: 6/7.0 ---epoch: 85/100--- mse-loss: 3.3183212280273438\n",
      "batch: 7/7.0 ---epoch: 85/100--- mse-loss: 3.29408597946167\n",
      "batch: 1/7.0 ---epoch: 86/100--- mse-loss: 3.1299171447753906\n",
      "batch: 2/7.0 ---epoch: 86/100--- mse-loss: 3.201150417327881\n",
      "batch: 3/7.0 ---epoch: 86/100--- mse-loss: 3.245957136154175\n",
      "batch: 4/7.0 ---epoch: 86/100--- mse-loss: 3.1880156993865967\n",
      "batch: 5/7.0 ---epoch: 86/100--- mse-loss: 3.1535701751708984\n",
      "batch: 6/7.0 ---epoch: 86/100--- mse-loss: 3.313570022583008\n",
      "batch: 7/7.0 ---epoch: 86/100--- mse-loss: 3.2145793437957764\n",
      "batch: 1/7.0 ---epoch: 87/100--- mse-loss: 3.156435489654541\n",
      "batch: 2/7.0 ---epoch: 87/100--- mse-loss: 3.283953905105591\n",
      "batch: 3/7.0 ---epoch: 87/100--- mse-loss: 3.2457358837127686\n",
      "batch: 4/7.0 ---epoch: 87/100--- mse-loss: 3.181861400604248\n",
      "batch: 5/7.0 ---epoch: 87/100--- mse-loss: 3.1516499519348145\n",
      "batch: 6/7.0 ---epoch: 87/100--- mse-loss: 3.1688766479492188\n",
      "batch: 7/7.0 ---epoch: 87/100--- mse-loss: 3.1560349464416504\n",
      "batch: 1/7.0 ---epoch: 88/100--- mse-loss: 3.126997470855713\n",
      "batch: 2/7.0 ---epoch: 88/100--- mse-loss: 3.095585823059082\n",
      "batch: 3/7.0 ---epoch: 88/100--- mse-loss: 3.166459083557129\n",
      "batch: 4/7.0 ---epoch: 88/100--- mse-loss: 3.1890385150909424\n",
      "batch: 5/7.0 ---epoch: 88/100--- mse-loss: 3.3136520385742188\n",
      "batch: 6/7.0 ---epoch: 88/100--- mse-loss: 3.267042636871338\n",
      "batch: 7/7.0 ---epoch: 88/100--- mse-loss: 3.136134386062622\n",
      "batch: 1/7.0 ---epoch: 89/100--- mse-loss: 3.158372640609741\n",
      "batch: 2/7.0 ---epoch: 89/100--- mse-loss: 3.18168568611145\n",
      "batch: 3/7.0 ---epoch: 89/100--- mse-loss: 3.209204912185669\n",
      "batch: 4/7.0 ---epoch: 89/100--- mse-loss: 3.1479272842407227\n",
      "batch: 5/7.0 ---epoch: 89/100--- mse-loss: 3.1415655612945557\n",
      "batch: 6/7.0 ---epoch: 89/100--- mse-loss: 3.1013941764831543\n",
      "batch: 7/7.0 ---epoch: 89/100--- mse-loss: 3.145534038543701\n",
      "batch: 1/7.0 ---epoch: 90/100--- mse-loss: 3.2013297080993652\n",
      "batch: 2/7.0 ---epoch: 90/100--- mse-loss: 3.108933687210083\n",
      "batch: 3/7.0 ---epoch: 90/100--- mse-loss: 3.1466546058654785\n",
      "batch: 4/7.0 ---epoch: 90/100--- mse-loss: 3.1825804710388184\n",
      "batch: 5/7.0 ---epoch: 90/100--- mse-loss: 3.1491341590881348\n",
      "batch: 6/7.0 ---epoch: 90/100--- mse-loss: 3.108551502227783\n",
      "batch: 7/7.0 ---epoch: 90/100--- mse-loss: 3.2517991065979004\n",
      "batch: 1/7.0 ---epoch: 91/100--- mse-loss: 3.300732374191284\n",
      "batch: 2/7.0 ---epoch: 91/100--- mse-loss: 3.2253260612487793\n",
      "batch: 3/7.0 ---epoch: 91/100--- mse-loss: 3.066236972808838\n",
      "batch: 4/7.0 ---epoch: 91/100--- mse-loss: 3.2364280223846436\n",
      "batch: 5/7.0 ---epoch: 91/100--- mse-loss: 3.190293073654175\n",
      "batch: 6/7.0 ---epoch: 91/100--- mse-loss: 3.1496164798736572\n",
      "batch: 7/7.0 ---epoch: 91/100--- mse-loss: 3.1494927406311035\n",
      "batch: 1/7.0 ---epoch: 92/100--- mse-loss: 3.1223506927490234\n",
      "batch: 2/7.0 ---epoch: 92/100--- mse-loss: 3.180178165435791\n",
      "batch: 3/7.0 ---epoch: 92/100--- mse-loss: 3.199401617050171\n",
      "batch: 4/7.0 ---epoch: 92/100--- mse-loss: 3.2219855785369873\n",
      "batch: 5/7.0 ---epoch: 92/100--- mse-loss: 3.208819627761841\n",
      "batch: 6/7.0 ---epoch: 92/100--- mse-loss: 3.134171962738037\n",
      "batch: 7/7.0 ---epoch: 92/100--- mse-loss: 3.2161922454833984\n",
      "batch: 1/7.0 ---epoch: 93/100--- mse-loss: 3.3668296337127686\n",
      "batch: 2/7.0 ---epoch: 93/100--- mse-loss: 3.221315383911133\n",
      "batch: 3/7.0 ---epoch: 93/100--- mse-loss: 3.109170436859131\n",
      "batch: 4/7.0 ---epoch: 93/100--- mse-loss: 3.288524627685547\n",
      "batch: 5/7.0 ---epoch: 93/100--- mse-loss: 3.1779229640960693\n",
      "batch: 6/7.0 ---epoch: 93/100--- mse-loss: 3.0848050117492676\n",
      "batch: 7/7.0 ---epoch: 93/100--- mse-loss: 3.3090598583221436\n",
      "batch: 1/7.0 ---epoch: 94/100--- mse-loss: 3.3965060710906982\n",
      "batch: 2/7.0 ---epoch: 94/100--- mse-loss: 3.222325086593628\n",
      "batch: 3/7.0 ---epoch: 94/100--- mse-loss: 3.0458242893218994\n",
      "batch: 4/7.0 ---epoch: 94/100--- mse-loss: 3.3314528465270996\n",
      "batch: 5/7.0 ---epoch: 94/100--- mse-loss: 3.3578357696533203\n",
      "batch: 6/7.0 ---epoch: 94/100--- mse-loss: 3.161473035812378\n",
      "batch: 7/7.0 ---epoch: 94/100--- mse-loss: 3.275667667388916\n",
      "batch: 1/7.0 ---epoch: 95/100--- mse-loss: 3.381535291671753\n",
      "batch: 2/7.0 ---epoch: 95/100--- mse-loss: 3.206235408782959\n",
      "batch: 3/7.0 ---epoch: 95/100--- mse-loss: 3.078125476837158\n",
      "batch: 4/7.0 ---epoch: 95/100--- mse-loss: 3.2342610359191895\n",
      "batch: 5/7.0 ---epoch: 95/100--- mse-loss: 3.114825487136841\n",
      "batch: 6/7.0 ---epoch: 95/100--- mse-loss: 3.1074092388153076\n",
      "batch: 7/7.0 ---epoch: 95/100--- mse-loss: 3.1737828254699707\n",
      "batch: 1/7.0 ---epoch: 96/100--- mse-loss: 3.091625690460205\n",
      "batch: 2/7.0 ---epoch: 96/100--- mse-loss: 3.1561214923858643\n",
      "batch: 3/7.0 ---epoch: 96/100--- mse-loss: 3.1862716674804688\n",
      "batch: 4/7.0 ---epoch: 96/100--- mse-loss: 3.1590118408203125\n",
      "batch: 5/7.0 ---epoch: 96/100--- mse-loss: 3.174792528152466\n",
      "batch: 6/7.0 ---epoch: 96/100--- mse-loss: 3.4099934101104736\n",
      "batch: 7/7.0 ---epoch: 96/100--- mse-loss: 3.1114425659179688\n",
      "batch: 1/7.0 ---epoch: 97/100--- mse-loss: 3.147569179534912\n",
      "batch: 2/7.0 ---epoch: 97/100--- mse-loss: 3.3596091270446777\n",
      "batch: 3/7.0 ---epoch: 97/100--- mse-loss: 3.2190024852752686\n",
      "batch: 4/7.0 ---epoch: 97/100--- mse-loss: 3.0705180168151855\n",
      "batch: 5/7.0 ---epoch: 97/100--- mse-loss: 3.164674997329712\n",
      "batch: 6/7.0 ---epoch: 97/100--- mse-loss: 3.1476688385009766\n",
      "batch: 7/7.0 ---epoch: 97/100--- mse-loss: 3.1105825901031494\n",
      "batch: 1/7.0 ---epoch: 98/100--- mse-loss: 3.2874343395233154\n",
      "batch: 2/7.0 ---epoch: 98/100--- mse-loss: 3.398639678955078\n",
      "batch: 3/7.0 ---epoch: 98/100--- mse-loss: 3.22186017036438\n",
      "batch: 4/7.0 ---epoch: 98/100--- mse-loss: 3.1269896030426025\n",
      "batch: 5/7.0 ---epoch: 98/100--- mse-loss: 3.3465065956115723\n",
      "batch: 6/7.0 ---epoch: 98/100--- mse-loss: 3.3697304725646973\n",
      "batch: 7/7.0 ---epoch: 98/100--- mse-loss: 3.0526633262634277\n",
      "batch: 1/7.0 ---epoch: 99/100--- mse-loss: 3.3361027240753174\n",
      "batch: 2/7.0 ---epoch: 99/100--- mse-loss: 3.2832694053649902\n",
      "batch: 3/7.0 ---epoch: 99/100--- mse-loss: 3.096139907836914\n",
      "batch: 4/7.0 ---epoch: 99/100--- mse-loss: 3.3823676109313965\n",
      "batch: 5/7.0 ---epoch: 99/100--- mse-loss: 3.2900567054748535\n",
      "batch: 6/7.0 ---epoch: 99/100--- mse-loss: 3.0290985107421875\n",
      "batch: 7/7.0 ---epoch: 99/100--- mse-loss: 3.3519089221954346\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "path = os.getcwd()\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(tf.__version__)\n",
    "print(device_lib.list_local_devices())\n",
    "# * 데이터셋\n",
    "# In[] SMOKE-CMAQ\n",
    "# CMAQ 연평균 PM농도\n",
    "pm2_5 = np.array([])\n",
    "\n",
    "for i in range(1,120):\n",
    "    a = 'ACONC.' + str(i)\n",
    "    nc = Dataset(a, 'r')\n",
    "    pm2_5 = np.append(pm2_5, np.array([[[nc.variables['PM2_5']]]]))\n",
    "    \n",
    "pm2_5 = np.reshape(pm2_5, [119,82,67])\n",
    "pm2_5 = pm2_5[:,8:-10,2:-1]\n",
    "pm2_5 = np.reshape(pm2_5, [119,64,64]) # convolution 연산을 위해\n",
    "\n",
    "pm = pm2_5\n",
    "\n",
    "\n",
    "y1 = np.array([])\n",
    "y2 = np.array([])\n",
    "y3 = np.array([])\n",
    "y4 = np.array([])\n",
    "y5 = np.array([])\n",
    "y6 = np.array([])\n",
    "\n",
    "for i in range(1,120):\n",
    "    a = 'EMIS_AVG.' + str(i)\n",
    "    nc = Dataset(a, 'r')\n",
    "    y1 = np.append(y1, np.array([[[nc.variables['SO2']]]]))\n",
    "    y2 = np.append(y2, np.array([[[nc.variables['PM2_5']]]]))\n",
    "    y3 = np.append(y3, np.array([[[nc.variables['NOx']]]]))\n",
    "    y4 = np.append(y4, np.array([[[nc.variables['VOCs']]]]))\n",
    "    y5 = np.append(y5, np.array([[[nc.variables['NH3']]]]))\n",
    "    y6 = np.append(y6, np.array([[[nc.variables['CO']]]]))\n",
    "    \n",
    "y1 = np.reshape(y1, [119,82,67])\n",
    "y1 = y1[:,8:-10,2:-1]\n",
    "y1 = np.reshape(y1, [119,64,64])\n",
    "y1_max = np.max(y1)\n",
    "y1_min = np.min(y1)\n",
    "\n",
    "y2 = np.reshape(y2, [119,82,67])\n",
    "y2 = y2[:,8:-10,2:-1]\n",
    "y2 = np.reshape(y2, [119,64,64])\n",
    "y2_max = np.max(y2)\n",
    "y2_min = np.min(y2)\n",
    "\n",
    "y3 = np.reshape(y3, [119,82,67])\n",
    "y3 = y3[:,8:-10,2:-1]\n",
    "y3 = np.reshape(y3, [119,64,64])\n",
    "y3_max = np.max(y3)\n",
    "y3_min = np.min(y3)\n",
    "\n",
    "y4 = np.reshape(y4, [119,82,67])\n",
    "y4 = y4[:,8:-10,2:-1]\n",
    "y4 = np.reshape(y4, [119,64,64])\n",
    "y4_max = np.max(y4)\n",
    "y4_min = np.min(y4)\n",
    "\n",
    "y5 = np.reshape(y5, [119,82,67])\n",
    "y5 = y5[:,8:-10,2:-1]\n",
    "y5 = np.reshape(y5, [119,64,64])\n",
    "y5_max = np.max(y5)\n",
    "y5_min = np.min(y5)\n",
    "    \n",
    "y6 = np.reshape(y6, [119,82,67])\n",
    "y6 = y6[:,8:-10,2:-1]\n",
    "y6 = np.reshape(y6, [119,64,64])\n",
    "\n",
    "\n",
    "def pm_data():\n",
    "    a = np.concatenate((y3,y1,y4,y5), axis=0) # PM2.5, SO2, NH3\n",
    "    a = np.reshape(a,[4,119,64,64]) # data generation for CNN ??\n",
    "    a = np.transpose(a, (1,2,3,0)) # CNN 학습을 위한 순서변경\n",
    "    print(np.shape(a))\n",
    "    \n",
    "    y = pm\n",
    "    \n",
    "    y = np.reshape(y, [119,64,64,1]) # CNN output data\n",
    "    x_train = a[0:70]\n",
    "    y_train = y[0:70]\n",
    "    x_test = a[70:]\n",
    "    pm2_5 = y[70:]\n",
    "    return x_train, y_train, x_test, pm2_5, a, y\n",
    "\n",
    "\n",
    "pm_data1 = pm_data()\n",
    "x_train, y_train, x_test, pm2_5, a, y = pm_data1[0], pm_data1[1], pm_data1[2], pm_data1[3], pm_data1[4], pm_data1[5]#, pm_data1[6], pm_data1[7] \n",
    "\n",
    "# * test_1\n",
    "# 매우 성능이 떨어지는 pre_weak_learner 사용하는 경우\n",
    "class CBR2d(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels,kernel_size = 3):\n",
    "        super(CBR2d, self).__init__()\n",
    "        self.cnn_layer = tf.keras.layers.Conv2D(out_channels, kernel_size, activation='relu', padding='same')\n",
    "        self.batch_layer = tf.keras.layers.BatchNormalization()\n",
    "              \n",
    "    def call(self, inputs):\n",
    "        cnn_feat_x = self.cnn_layer(inputs)\n",
    "        batched_feat = self.batch_layer(cnn_feat_x)\n",
    "        return batched_feat\n",
    "\n",
    "class Weakcnn(tf.keras.Model): \n",
    "    def __init__(self,): \n",
    "        super(Weakcnn, self).__init__()\n",
    "\n",
    "        self.layer_1 = CBR2d(out_channels = 32) \n",
    "        self.layer_2 = CBR2d(out_channels= 128)\n",
    "        self.layer_3 = CBR2d(out_channels = 128)\n",
    "        self.layer_4 = CBR2d(out_channels = 32)\n",
    "        # self.layer_5 = CBR2d(out_channels = 512)\n",
    "        # self.layer_6 = CBR2d(out_channels = 256)\n",
    "        # self.layer_7 = CBR2d(out_channels = 128)\n",
    "        # self.layer_8 = CBR2d(out_channels = 64)\n",
    "\n",
    "\n",
    "\n",
    "        self.outlayer = tf.keras.layers.Conv2D(1,kernel_size = 1)\n",
    "              \n",
    "    def call(self, input): \n",
    "      \n",
    "        x = self.layer_1(input)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        # x = self.layer_5(x)\n",
    "        # x = self.layer_6(x)\n",
    "        # x = self.layer_7(x)\n",
    "        # x = self.layer_8(x)\n",
    "        x = self.outlayer(x)\n",
    "        return x\n",
    "class Restree():\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: float,\n",
    "        i_depth: float,\n",
    "        minimum_sample_leaf: float,\n",
    "        y_val: np.array,\n",
    "        x_val: np.array,\n",
    "\n",
    "        res_y_val: np.array,\n",
    "\n",
    "        is_terminal: bool,\n",
    "\n",
    "        kernel_size: list,\n",
    "        kernel_n: int,\n",
    "\n",
    "        input_shape: list,\n",
    "        # pre_weak_net: Weakcnn,\n",
    "        pre_weak_net = None,\n",
    "\n",
    "        feature_list = [],\n",
    "\n",
    "\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.minimum_sample_leaf = minimum_sample_leaf\n",
    "        self.i_depth = i_depth\n",
    "        self.is_terminal = is_terminal\n",
    "\n",
    "        self.res_y_val = res_y_val\n",
    "        if self.i_depth != 0: # 사전모델 있는 첫노드가 아닌경우 이미 y값 잔차이기 때문에 그냥 쓰면됨\n",
    "            # self.y_val = y_val   ### 여기서 모델 시작되자마자 y_val을 사전학습모델과 y와의 잔차로 피팅하기##\n",
    "            # self.y_val = y_val - pre_weak_net(x_train).numpy() \n",
    "            self.y_val = y_val\n",
    "            \n",
    "        else:\n",
    "            self.y_val = y_val - self.res_y_val\n",
    "        # self.y_val = y_val - self.res_y_val\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        self.x_val = x_val\n",
    "        self.best_feature = None\n",
    "        self.best_feature_value = None\n",
    "\n",
    "        self.kernel_size = kernel_size#[3,3]\n",
    "        self.kernel_n = kernel_n# 30\n",
    "        self.input_shape = input_shape # [70,64,64,3]\n",
    "\n",
    "        self.pred_val = None # 갖는 이미지의 평균값, 피팅이 완료되면 y_val, x_val은 버리도록(메모리낭비)\n",
    "\n",
    "        self.l_tree = None\n",
    "        self.r_tree = None\n",
    "\n",
    "        ax1 = np.random.randint(input_shape[1] - kernel_size[0], size=kernel_n).reshape(-1,1)\n",
    "        ax2 = np.random.randint(input_shape[2] - kernel_size[1], size=kernel_n).reshape(-1,1)\n",
    "        ax3 = np.random.randint(input_shape[3], size=kernel_n).reshape(-1,1)\n",
    "\n",
    "        self.kernel_cords = np.concatenate([ax1,ax2,ax3],axis = 1)\n",
    "\n",
    "        # self.weak_net = pre_weak_net\n",
    "\n",
    "\n",
    "    def obj_fun_bhattacharyya_dist(self, l_values, r_values):\n",
    "        \n",
    "\n",
    "        l_arr = np.float32(l_values)\n",
    "        r_arr = np.float32(r_values)\n",
    "\n",
    "        max_val = np.max(np.concatenate([l_arr,r_arr]))\n",
    "\n",
    "        hist_cv_l = cv2.calcHist([l_arr],[0],None,[40],[0,max_val])\n",
    "        hist_cv_r = cv2.calcHist([r_arr],[0],None,[40] ,[0,max_val])    #[100] 빈수, [0,256]값 범위 \n",
    "\n",
    "        dist = cv2.compareHist(hist_cv_l, hist_cv_r, cv2.HISTCMP_BHATTACHARYYA) #작을수록 유사도큼\n",
    "\n",
    "        return dist * -1\n",
    "\n",
    "    # def fit_weak_layer(self):\n",
    "    #     x_train_tensor = tf.convert_to_tensor(self.x_val,dtype = float)\n",
    "    #     # x_test_tensor = tf.convert_to_tensor(x_test,dtype = float)\n",
    "    #     y_train_resize_tensor = tf.convert_to_tensor(self.y_val,dtype = float)\n",
    "    #     # y_test_resize_tensor = tf.convert_to_tensor(pm2_5,dtype = float)\n",
    "\n",
    "    #     batch_size = self.minimum_sample_leaf\n",
    "    #     epoch = 100\n",
    "    #     buffer_len = len(x_train_tensor)\n",
    "    #     n_run = np.ceil(len(x_train_tensor)/batch_size)\n",
    "\n",
    "    #     train_dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x_train_tensor), tf.data.Dataset.from_tensor_slices(y_train_resize_tensor),))\n",
    "    #     shuffled_dataset = train_dataset.shuffle(buffer_size=buffer_len)\n",
    "\n",
    "    #     @tf.function()\n",
    "    #     def train_step(input, label, network, loss_fun,opt):\n",
    "        \n",
    "    #         with tf.GradientTape() as tape:\n",
    "    #             ##로스 계산\n",
    "    #             pred = network(input)\n",
    "                \n",
    "    #             loss = loss_fun(label, pred)\n",
    "    #         grads = tape.gradient(loss, network.trainable_variables)\n",
    "    #         opt.apply_gradients(zip(grads, network.trainable_variables))\n",
    "    #         return loss\n",
    "\n",
    "    #     weak_net = Weakcnn()\n",
    "    #     weak_net(x_train_tensor[:2]).numpy()  # 콜함수 한번 실행시켜서 초기 웨이트 설정\n",
    "    #     weak_net.set_weights(self.pre_weak_weights) # 프리트레인 웨이트 할당\n",
    "\n",
    "        \n",
    "    #     loss = tf.keras.losses.MeanSquaredError()\n",
    "    #     optimizer = tf.keras.optimizers.Adam(0.0001)\n",
    "\n",
    "    #     for epoc in range(epoch):\n",
    "    #         count = 0\n",
    "\n",
    "    #         for batch in shuffled_dataset.batch(batch_size).take(n_run):\n",
    "\n",
    "    #             count += 1\n",
    "    #             mse_loss = train_step(batch[0],batch[1], weak_net,loss,optimizer).numpy()\n",
    "    #             print(f\"batch: {count}/{n_run} ---epoch: {epoc}/{epoch}--- mse-loss: {mse_loss}\")\n",
    "\n",
    "    #     self.weak_net = weak_net\n",
    "       \n",
    "\n",
    "\n",
    "    def fit(self, ):\n",
    "        best_score = None\n",
    "        for cords in self.kernel_cords:  #컬럼_피처 루프\n",
    "            \n",
    "            feat_data = self.x_val[:,cords[0]:cords[0]+self.kernel_size[0],cords[1]:cords[1]+self.kernel_size[1],cords[2]]\n",
    "\n",
    "\n",
    "            if len(set(feat_data.mean(axis = (1,2)))) == 1:\n",
    "\n",
    "                ## 모든 kernel_cords에서 특징이 똑같은루프만 나오면 루프를 다 돌아도 best_score = None 상태로 남음\n",
    "                ## 이상태에서 아래 피팅 부분으로 돌아가면 에러남\n",
    "                continue\n",
    "            elif len(set(feat_data.mean(axis = (1,2)))) >= 100:\n",
    "                selected_feat_vals = list(np.random.choice(list(set(feat_data.mean(axis = (1,2)))),100, replace = False))\n",
    "            else:\n",
    "                selected_feat_vals = list(set(feat_data.mean(axis = (1,2))))\n",
    "\n",
    "        \n",
    "\n",
    "            for j in selected_feat_vals: #한 컬럼내에서 분류기준나누기 위한 루프\n",
    "\n",
    "                left_ind = feat_data.mean(axis = (1,2)) < j\n",
    "\n",
    "                y_left = self.y_val[left_ind]\n",
    "                y_right = self.y_val[~left_ind]\n",
    "\n",
    "                if len(y_left) * len(y_right) != 0:\n",
    "\n",
    "                    if self.best_feature is None:\n",
    "                        self.best_feature = cords\n",
    "                        self.best_feature_value = j\n",
    "                        best_score = self.obj_fun_bhattacharyya_dist(y_left, y_right)\n",
    "                        \n",
    "                    else:\n",
    "                        new_score = self.obj_fun_bhattacharyya_dist(y_left, y_right)\n",
    "                        if new_score < best_score:\n",
    "                            self.best_feature = cords\n",
    "                            self.best_feature_value = j\n",
    "                            best_score = new_score\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        if best_score is None:\n",
    "            self.is_terminal = True\n",
    "            # self.fit_weak_layer()\n",
    "            \n",
    "            return None\n",
    "        \n",
    "                        \n",
    "\n",
    "\n",
    "        if self.max_depth >= self.i_depth:\n",
    "            # 찾은 최적값으로 좌우 할당\n",
    "            x_val_meaned_bykernel = self.x_val[:,self.best_feature[0]:self.best_feature[0]+self.kernel_size[0],self.best_feature[1]:self.best_feature[1]+self.kernel_size[1],self.best_feature[2]].mean(axis = (1,2))\n",
    "    \n",
    "            left_ind = x_val_meaned_bykernel < self.best_feature_value\n",
    "            \n",
    "            y_left = self.y_val[left_ind]\n",
    "            y_right = self.y_val[~left_ind]\n",
    "\n",
    "            x_left = self.x_val[left_ind]\n",
    "            x_right = self.x_val[~left_ind]\n",
    "\n",
    "            if len(y_left) > self.minimum_sample_leaf:\n",
    "                # self.weak_net = pre_weak_net\n",
    "                self.l_tree = Restree(res_y_val = self.res_y_val[left_ind],max_depth = self.max_depth,i_depth = self.i_depth + 1, minimum_sample_leaf = self.minimum_sample_leaf, x_val = x_left, y_val = y_left, is_terminal=False, kernel_size=self.kernel_size, kernel_n=self.kernel_n, input_shape=self.input_shape)\n",
    "                self.l_tree.fit()\n",
    "               \n",
    "            else:\n",
    "                #마지막 리프라서 피팅할필요 없이 y_val 평균으로 예측만 하면 됨\n",
    "                #여기서 y_val이 없는 경우가 있음 이때 nan 리턴되서 에러, 먼저 해결\n",
    "                if len(y_left) != 0 :\n",
    "                    self.l_tree = Restree(res_y_val = self.res_y_val[left_ind],max_depth = self.max_depth,i_depth = self.i_depth + 1, minimum_sample_leaf = self.minimum_sample_leaf, x_val = x_left, y_val = y_left, is_terminal=True, kernel_size=self.kernel_size, kernel_n=self.kernel_n, input_shape=self.input_shape)\n",
    "                    # self.l_tree.fit_weak_layer()\n",
    "                else:\n",
    "                    self.is_terminal = True\n",
    "                    # self.fit_weak_layer()\n",
    "            if len(y_right) > self.minimum_sample_leaf:\n",
    "                self.r_tree = Restree(res_y_val = self.res_y_val[~left_ind],max_depth = self.max_depth,i_depth = self.i_depth + 1, minimum_sample_leaf = self.minimum_sample_leaf, x_val = x_right, y_val = y_right, is_terminal=False, kernel_size=self.kernel_size, kernel_n=self.kernel_n, input_shape=self.input_shape)\n",
    "                self.r_tree.fit()\n",
    "            else:\n",
    "                #마지막 리프라서 피팅할필요 없이 y_val 평균으로 예측만 하면 됨\n",
    "                #여기서 y_val이 없는 경우가 있음 이때 nan 리턴되서 에러, 먼저 해결\n",
    "                if len(y_right) !=0:\n",
    "                    self.r_tree = Restree(res_y_val = self.res_y_val[~left_ind],max_depth = self.max_depth,i_depth = self.i_depth + 1, minimum_sample_leaf = self.minimum_sample_leaf, x_val = x_right, y_val = y_right, is_terminal=True, kernel_size=self.kernel_size, kernel_n=self.kernel_n, input_shape=self.input_shape)\n",
    "                    # self.r_tree.fit_weak_layer()\n",
    "                else:\n",
    "                    self.is_terminal = True\n",
    "                    # self.fit_weak_layer()\n",
    "        else:\n",
    "            self.is_terminal = True\n",
    "            # self.fit_weak_layer()\n",
    "\n",
    "    def predict(self ,x_arr):\n",
    "        # model\n",
    "        kernel_x = x_arr[:,self.best_feature[0]:self.best_feature[0]+self.kernel_size[0],self.best_feature[1]:self.best_feature[1]+self.kernel_size[1],self.best_feature[2]].mean(axis = (1,2))\n",
    "        left_ind = kernel_x < self.best_feature_value\n",
    "\n",
    "        input_shape = x_arr.shape\n",
    "\n",
    "        pred_val = np.zeros(shape=(input_shape[0],input_shape[1],input_shape[2],1))\n",
    "        \n",
    "        x_arr_left = x_arr[left_ind]\n",
    "        x_arr_right = x_arr[~left_ind]\n",
    "\n",
    "        if self.l_tree.is_terminal:\n",
    "            # pred_l = self.l_tree.weak_net(x_arr_left)\n",
    "            pred_l = self.l_tree.y_val.mean(axis = 0)\n",
    "        else:\n",
    "            pred_l = self.l_tree.predict(x_arr_left)\n",
    "\n",
    "        if self.r_tree.is_terminal:\n",
    "            # pred_r = self.r_tree.weak_net(x_arr_right)\n",
    "            pred_r = self.r_tree.y_val.mean(axis = 0)\n",
    "        else:\n",
    "            pred_r = self.r_tree.predict(x_arr_right)\n",
    "        \n",
    "        pred_val[left_ind] = pred_l\n",
    "        pred_val[~left_ind] = pred_r\n",
    "\n",
    "\n",
    "        return pred_val\n",
    "\n",
    "\n",
    "\n",
    "    def get_tree_structure(self):\n",
    "        feat_list = []\n",
    "        def get_info_dic(i_tree):\n",
    "            result = {\n",
    "            'best_feature': i_tree.best_feature,\n",
    "            'best_feature_value': i_tree.best_feature_value,\n",
    "            'terminal': i_tree.is_terminal,\n",
    "            'depth': i_tree.i_depth,\n",
    "            }\n",
    "            if result['best_feature'] is not None:\n",
    "                feat_list.append(result['best_feature'])\n",
    "            \n",
    "            if i_tree.l_tree is not None:\n",
    "                result['l_tree'] = get_info_dic(i_tree.l_tree)\n",
    "            \n",
    "            if i_tree.r_tree is not None:\n",
    "                result['r_tree'] = get_info_dic(i_tree.r_tree)\n",
    "\n",
    "            return result\n",
    "        \n",
    "        info = get_info_dic(self)\n",
    "        return info, feat_list\n",
    "class bagging_cnn():\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_depth: float,\n",
    "        i_depth: float,\n",
    "        minimum_sample_leaf: float,\n",
    "        y_val: np.array,\n",
    "        x_val: np.array,\n",
    "        is_terminal: bool,\n",
    "\n",
    "        kernel_size: list,\n",
    "        kernel_n: int,\n",
    "\n",
    "        input_shape: list,\n",
    "\n",
    "        n_tree: int,\n",
    "\n",
    "        pre_weak_net: Weakcnn,\n",
    "    ):\n",
    "        self.max_depth = max_depth\n",
    "        self.minimum_sample_leaf = minimum_sample_leaf\n",
    "        self.i_depth = i_depth\n",
    "        self.is_terminal = is_terminal\n",
    "\n",
    "        self.y_val = y_val\n",
    "        self.x_val = x_val\n",
    "\n",
    "        self.kernel_size = kernel_size#[3,3]\n",
    "        self.kernel_n = kernel_n# 30\n",
    "        self.input_shape = input_shape # [70,64,64,3]\n",
    "        \n",
    "        self.tree_bootstrap = []\n",
    "        self.n_tree = n_tree\n",
    "\n",
    "        self.pre_weak_net = pre_weak_net\n",
    "\n",
    "    def bagging_prediction(self,x_vals):\n",
    "        print(x_vals.shape)\n",
    "        preds = np.mean([ind_tree.predict(x_vals) for ind_tree in self.tree_bootstrap], axis = 0)\n",
    "        preds = preds + self.pre_weak_net(x_vals).numpy()\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "    def bagging_train(self,):\n",
    "        for i in range(self.n_tree):\n",
    "            print(i)\n",
    "            data_len = len(self.x_val)\n",
    "            sub_sample_ind = np.random.choice(range(data_len),data_len)\n",
    "            x_val_bootstraped = self.x_val[sub_sample_ind]\n",
    "            y_val_bootstraped = self.y_val[sub_sample_ind]\n",
    "\n",
    "            i_tree = Restree(res_y_val = self.pre_weak_net(x_val_bootstraped).numpy(),pre_weak_net = self.pre_weak_net,max_depth = self.max_depth,i_depth = 0, minimum_sample_leaf = self.minimum_sample_leaf, x_val = x_val_bootstraped, y_val = y_val_bootstraped, is_terminal=False, kernel_size=self.kernel_size, kernel_n=self.kernel_n, input_shape=x_val_bootstraped.shape)\n",
    "            i_tree.fit()\n",
    "            self.tree_bootstrap.append(i_tree)\n",
    "           \n",
    "    \n",
    "#프리트레인 weak 네트워크\n",
    "x_train_tensor = tf.convert_to_tensor(x_train,dtype = float)\n",
    "# x_test_tensor = tf.convert_to_tensor(x_test,dtype = float)\n",
    "y_train_resize_tensor = tf.convert_to_tensor(y_train,dtype = float)\n",
    "# y_test_resize_tensor = tf.convert_to_tensor(pm2_5,dtype = float)\n",
    "\n",
    "batch_size = 10\n",
    "epoch = 100\n",
    "buffer_len = len(x_train_tensor)\n",
    "n_run = np.ceil(len(x_train_tensor)/batch_size)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x_train_tensor), tf.data.Dataset.from_tensor_slices(y_train_resize_tensor),))\n",
    "shuffled_dataset = train_dataset.shuffle(buffer_size=buffer_len)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(input, label, network, loss_fun,opt):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        ##로스 계산\n",
    "        pred = network(input)\n",
    "        \n",
    "        loss = loss_fun(label, pred)\n",
    "    grads = tape.gradient(loss, network.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, network.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "weak_net = Weakcnn()\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoc in range(epoch):\n",
    "    count = 0\n",
    "\n",
    "    for batch in shuffled_dataset.batch(batch_size).take(n_run):\n",
    "\n",
    "        count += 1\n",
    "        mse_loss = train_step(batch[0],batch[1], weak_net,loss,optimizer).numpy()\n",
    "        print(f\"batch: {count}/{n_run} ---epoch: {epoc}/{epoch}--- mse-loss: {mse_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "s_count = 0 # foreground 샘플에서만 만난 피처, pos 구할때는 1 빼준다\n",
    "n_count = 2  # 지나면서 만난 전체 피처\n",
    "\n",
    "def weight(sc,nc):\n",
    "    pw = 0\n",
    "    nw = 0\n",
    "    if sc == 0:\n",
    "        if nc != 0:\n",
    "            nw = -1*(math.factorial(sc) * math.factorial(nc - sc - 1))/math.factorial(nc)\n",
    "        return pw,nw\n",
    "    if sc !=0:\n",
    "        pw = (math.factorial(sc-1) * math.factorial(nc - (sc -1) - 1))/math.factorial(nc)\n",
    "    if sc != nc:\n",
    "        nw = -1*(math.factorial(sc) * math.factorial(nc - sc - 1))/math.factorial(nc)\n",
    "\n",
    "    return pw, nw\n",
    "\n",
    "weight(s_count,n_count)\n",
    "\n",
    "\n",
    "def treeshap_dynamic(x_foreground, x_background, tree):\n",
    "\n",
    "    def recurse(node_tree, n_c, s_c, foreground_check, background_check):\n",
    "        \n",
    "\n",
    "        if node_tree.is_terminal:\n",
    "            pw, nw = weight(s_c,n_c)\n",
    "            val_sum = (node_tree.y_val + node_tree.res_y_val).mean()    #여기를 만약에 sum으로하면 리프가 갖는 데이터셋 개수에 따라서 값차이가 생기므로 평균으로 처리해야함\n",
    "            \n",
    "            return val_sum * pw, val_sum * nw\n",
    "        \n",
    "        \n",
    "\n",
    "        kernel_xf_val = x_foreground[node_tree.best_feature[0]:node_tree.best_feature[0]+node_tree.kernel_size[0],node_tree.best_feature[1]:node_tree.best_feature[1]+node_tree.kernel_size[1],node_tree.best_feature[2]].mean()\n",
    "        kernel_xb_val = x_background[node_tree.best_feature[0]:node_tree.best_feature[0]+node_tree.kernel_size[0],node_tree.best_feature[1]:node_tree.best_feature[1]+node_tree.kernel_size[1],node_tree.best_feature[2]].mean()\n",
    "\n",
    "        xf_child = node_tree.l_tree if kernel_xf_val < node_tree.best_feature_value else node_tree.r_tree\n",
    "        xb_child = node_tree.l_tree if kernel_xb_val < node_tree.best_feature_value else node_tree.r_tree\n",
    "            \n",
    "        if foreground_check[str(node_tree.best_feature)] > 0:\n",
    "            return recurse(xf_child,n_c,s_c, foreground_check, background_check)\n",
    "        \n",
    "        if background_check[str(node_tree.best_feature)] > 0:\n",
    "            return recurse(xb_child,n_c,s_c, foreground_check, background_check)\n",
    "\n",
    "        if xf_child == xb_child:\n",
    "            return recurse(xb_child,n_c,s_c, foreground_check, background_check)\n",
    "\n",
    "        if xf_child != xb_child:\n",
    "            # foreground search\n",
    "            foreground_check[str(node_tree.best_feature)] += 1\n",
    "            posf, negf = recurse(xf_child,n_c + 1,s_c + 1, foreground_check, background_check)\n",
    "            foreground_check[str(node_tree.best_feature)] -= 1\n",
    "\n",
    "\n",
    "            # background search\n",
    "            background_check[str(node_tree.best_feature)] += 1\n",
    "            posb, negb = recurse(xb_child,n_c + 1,s_c, foreground_check, background_check)\n",
    "            background_check[str(node_tree.best_feature)] -= 1\n",
    "\n",
    "            # add phi\n",
    "            phi_dic[str(node_tree.best_feature)] += (posf + negb)\n",
    "            \n",
    "            return posf + posb, negf + negb\n",
    "    \n",
    "    recurse(tree, 0, 0, feature_dic_forecheck, feature_dic_backcheck)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 27min 44s\n",
      "Wall time: 30min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [10,10]\n",
    "k_n = 30\n",
    "n_weak_tree = 100\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 29min 9s\n",
      "Wall time: 32min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [20,20]\n",
    "k_n = 30\n",
    "n_weak_tree = 100\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 27min 40s\n",
      "Wall time: 16h 41min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [30,30]\n",
    "k_n = 30\n",
    "n_weak_tree = 100\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 14min 26s\n",
      "Wall time: 17min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [10,10]\n",
    "k_n = 30\n",
    "n_weak_tree = 50\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 15min 36s\n",
      "Wall time: 20min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [20,20]\n",
    "k_n = 30\n",
    "n_weak_tree = 50\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 16min 22s\n",
      "Wall time: 19min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [30,30]\n",
    "k_n = 30\n",
    "n_weak_tree = 50\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 50min 1s\n",
      "Wall time: 56min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [10,10]\n",
    "k_n = 30\n",
    "n_weak_tree = 200\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 55min 16s\n",
      "Wall time: 3h 24min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [20,20]\n",
    "k_n = 30\n",
    "n_weak_tree = 200\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "(1, 64, 64, 4)\n",
      "CPU times: total: 54min 27s\n",
      "Wall time: 10h 48min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "min_sam_leaf = 5\n",
    "k_size = [30,30]\n",
    "k_n = 30\n",
    "n_weak_tree = 200\n",
    "\n",
    "\n",
    "bagging = bagging_cnn(\n",
    "    max_depth= 5,\n",
    "    i_depth= 0,\n",
    "    minimum_sample_leaf= min_sam_leaf,\n",
    "    y_val= y_train,\n",
    "    x_val= x_train,\n",
    "    is_terminal= False,\n",
    "\n",
    "    kernel_size= k_size,\n",
    "    kernel_n= k_n,\n",
    "\n",
    "    input_shape= x_train.shape,\n",
    "\n",
    "    n_tree= n_weak_tree,\n",
    "    pre_weak_net= weak_net,\n",
    "            )\n",
    "\n",
    "bagging.bagging_train()\n",
    "\n",
    "\n",
    "for x_ind in range(len(x_train)):\n",
    "    contrib_map = np.zeros(x_train[x_ind].shape)\n",
    "\n",
    "    for j in range(n_weak_tree):\n",
    "        target_tree = bagging.tree_bootstrap[j]\n",
    "        structure, feature_list = target_tree.get_tree_structure()\n",
    "    # predict(self ,x_arr)\n",
    "        # 기여도는 모든 background셋에 대해 더한 후 마지막에 개수로 평균\n",
    "        phi_dic = {}\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] = 0\n",
    "\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # 새로운 백그라운드 샘플하고 시행시에는 check 딕셔너리 초기화\n",
    "            feature_dic_forecheck = {}\n",
    "            feature_dic_backcheck = {}\n",
    "            for i in range(len(feature_list)):\n",
    "                feature_dic_forecheck[str(feature_list[i])] = 0\n",
    "                feature_dic_backcheck[str(feature_list[i])] = 0\n",
    "            \n",
    "            \n",
    "            treeshap_dynamic(x_train[x_ind], x_train[i], target_tree)\n",
    "        \n",
    "        # background에 대해 모두 더했으므로 background셋 크기로 나눠서 평균\n",
    "        for i in range(len(feature_list)):\n",
    "            phi_dic[str(feature_list[i])] /= len(x_train)\n",
    "        \n",
    "        \n",
    "        for feat in feature_list:\n",
    "            contrib_map[feat[0]:feat[0] + k_size[0],feat[1]:feat[1] + k_size[0],feat[2]] += phi_dic[str(feat)]\n",
    "        # print(phi_dic,)\n",
    "    contrib_map /= n_weak_tree\n",
    "\n",
    "    # bagging.tree_bootstrap[0].y_val[0]\n",
    "    fig = plt.figure(figsize = (10,25))\n",
    "            \n",
    "    ax1_1 = fig.add_subplot(5,2,1)\n",
    "    ax1_1.title.set_text('NOx_contrib') \n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map.mean() !=0 else None,ax = ax1_1)\n",
    "    # sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm(),ax = ax1_1)\n",
    "    sns.heatmap(contrib_map[:,:,0][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,0].mean() !=0 else None ,ax = ax1_1)\n",
    "\n",
    "    ax1_2 = fig.add_subplot(5,2,2)\n",
    "    ax1_2.title.set_text('NOx')\n",
    "    sns.heatmap(x_train[x_ind][:,:,0][::-1], cmap = 'jet',ax = ax1_2)\n",
    "\n",
    "    ax2_1 = fig.add_subplot(5,2,3)\n",
    "    ax2_1.title.set_text('SO2_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,1][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,1].mean() !=0 else None ,ax = ax2_1)\n",
    "\n",
    "    ax2_2 = fig.add_subplot(5,2,4)\n",
    "    ax2_2.title.set_text('SO2')\n",
    "    sns.heatmap(x_train[x_ind][:,:,1][::-1], cmap = 'jet',ax = ax2_2)\n",
    "\n",
    "    ax3_1 = fig.add_subplot(5,2,5)\n",
    "    ax3_1.title.set_text('VOCs_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,2][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,2].mean() !=0 else None ,ax = ax3_1)\n",
    "\n",
    "    ax3_2 = fig.add_subplot(5,2,6)\n",
    "    ax3_2.title.set_text('VOCs')\n",
    "    sns.heatmap(x_train[x_ind][:,:,2][::-1], cmap = 'jet',ax = ax3_2)\n",
    "\n",
    "    ax4_1 = fig.add_subplot(5,2,7)\n",
    "    ax4_1.title.set_text('NH3_contrib')\n",
    "    sns.heatmap(contrib_map[:,:,3][::-1], cmap = 'jet',norm = matplotlib.colors.CenteredNorm() if contrib_map[:,:,3].mean() !=0 else None ,ax = ax4_1)\n",
    "\n",
    "    ax4_2 = fig.add_subplot(5,2,8)\n",
    "    ax4_2.title.set_text('NH3')\n",
    "    sns.heatmap(x_train[x_ind][:,:,3][::-1], cmap = 'jet',ax = ax4_2)\n",
    "\n",
    "    ax5_1 = fig.add_subplot(5,2,9)\n",
    "    ax5_1.title.set_text('pred')\n",
    "    sns.heatmap(bagging.bagging_prediction(x_train[x_ind:x_ind+1])[0,:,:,0][::-1], cmap = 'jet',ax = ax5_1,vmin=0, vmax=80)\n",
    "\n",
    "    ax5_2 = fig.add_subplot(5,2,10)\n",
    "    ax5_2.title.set_text('true')\n",
    "    sns.heatmap(y_train[x_ind,:,:,0][::-1], cmap = 'jet',ax = ax5_2,vmin=0, vmax=80)\n",
    "\n",
    "    plt.savefig(f\"plots/contrib3/{x_ind}_{k_size}_{k_n}_{n_weak_tree}_contrib.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dfcec166549515651fe6305b8170d114ac82791493775815ad403a25f333b28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
